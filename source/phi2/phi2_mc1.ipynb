{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04afdb07-3e5d-460f-968f-0be2e71fab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69ed86e9-5da5-43aa-b0dd-65426d877cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_DATASETS_CACHE'] = '/work/pi_dhruveshpate_umass_edu/amalgonde_umass_edu'\n",
    "os.environ['CONDA_PKGS_DIRS'] = '/work/pi_dhruveshpate_umass_edu/amalgonde_umass_edu'\n",
    "os.environ['CONDA_ENVS_PATH'] = '/work/pi_dhruveshpate_umass_edu/amalgonde_umass_edu'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/work/pi_dhruveshpate_umass_edu/amalgonde_umass_edu'\n",
    "os.environ['HF_HOME'] = '/work/pi_dhruveshpate_umass_edu/amalgonde_umass_edu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f89b9fb-af30-4492-9a0e-91a6d4fe0e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amalgonde_umass_edu/.conda/envs/hw1/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce5f6386-fc84-4c00-b424-e05b67834e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-PCIE-16GB'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3df1e92a-df79-45c2-9a32-c895e232346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n",
      "- configuration_phi.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model max length - 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n",
      "- modeling_phi.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ffc0997fffa45d39c083e447271f7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99834fa9709c4ce086373d604e29b84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/phi-2\"\n",
    "# model_name = \"google/gemma-2b\"\n",
    "\n",
    "# config = AutoConfig.from_pretrained(model_name)\n",
    "# config.max_seq_len = 4096\n",
    "# config.max_answer_len= 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"Model max length - {tokenizer.model_max_length}\")\n",
    "# tokenizer.model_max_length = 4096\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             # max_length=4096,\n",
    "                                             trust_remote_code=True,\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\",\n",
    "                                             # load_in_8bit=True\n",
    "                                            )\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef0502-32b4-454b-83c4-c426c60d2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST\n",
    "\n",
    "# input_text = \"Write me a poem about Machine Learning.\"\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "# # input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, padding='max_length').to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(**input_ids, max_new_tokens=1024)\n",
    "# print(type(tokenizer.decode(outputs[0])), tokenizer.decode(outputs[0])) #.replace('<pad>', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623e9604-1070-4e0a-a116-6ce0196fdf3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prep Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c82c3-b0fa-4d70-ad05-42146b7e8438",
   "metadata": {},
   "source": [
    "### Entity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a1f4dab-2ed9-4dec-b0a8-0c5a4d66c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/work/pi_dhruveshpate_umass_edu/project_19/astha/696DS-named-entity-extraction-and-linking-for-KG-construction/code/mc1/mc1_preprocess/\"\n",
    "file = \"mc1_chunked_data.json\"\n",
    "\n",
    "with open(os.path.join(data_path, file), 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1168b986-c4f6-463c-a3f3-47243bf92a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data), data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6dfe56-2ece-4d6f-b1a9-40eee83dd37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data['Documents']), data['Documents'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158d23cc-a4f5-498e-b579-9b037940d60d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Relation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c09b28e-f48a-4980-9ce8-689ef36adf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1339, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_key</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>entities</th>\n",
       "      <th>latency</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "      <th>processed_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\\nKyrla Halford; University of Abila; Kronos; ...</td>\n",
       "      <td>1.578987</td>\n",
       "      <td>732</td>\n",
       "      <td>55</td>\n",
       "      <td>Kyrla Halford; University of Abila; Kronos; Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\n- Sri Lanka\\n- 30 June 1972\\n- Udawalawe N...</td>\n",
       "      <td>3.012619</td>\n",
       "      <td>726</td>\n",
       "      <td>107</td>\n",
       "      <td>Sri Lanka; 30 June 1972; Udawalawe National Pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>\\n\\nElodis; Kronos; 1997; grassroots effort; c...</td>\n",
       "      <td>7.108019</td>\n",
       "      <td>699</td>\n",
       "      <td>256</td>\n",
       "      <td>Elodis; Kronos; 1997; grassroots effort; conta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Elodis; City Council; Kronos; Health Center; ...</td>\n",
       "      <td>7.176608</td>\n",
       "      <td>698</td>\n",
       "      <td>256</td>\n",
       "      <td>Elodis; City Council; Kronos; Health Center; C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Henk Bodrogi; Kronos; Protectors of Kronos; S...</td>\n",
       "      <td>7.168755</td>\n",
       "      <td>743</td>\n",
       "      <td>256</td>\n",
       "      <td>Henk Bodrogi; Kronos; Protectors of Kronos; SM...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_key  chunk_index                                           entities  \\\n",
       "0        0            0  \\nKyrla Halford; University of Abila; Kronos; ...   \n",
       "1        0            1  \\n\\n- Sri Lanka\\n- 30 June 1972\\n- Udawalawe N...   \n",
       "2        0            2  \\n\\nElodis; Kronos; 1997; grassroots effort; c...   \n",
       "3        0            3   Elodis; City Council; Kronos; Health Center; ...   \n",
       "4        0            4   Henk Bodrogi; Kronos; Protectors of Kronos; S...   \n",
       "\n",
       "    latency  input_tokens  output_tokens  \\\n",
       "0  1.578987           732             55   \n",
       "1  3.012619           726            107   \n",
       "2  7.108019           699            256   \n",
       "3  7.176608           698            256   \n",
       "4  7.168755           743            256   \n",
       "\n",
       "                                  processed_entities  \n",
       "0  Kyrla Halford; University of Abila; Kronos; Pr...  \n",
       "1  Sri Lanka; 30 June 1972; Udawalawe National Pa...  \n",
       "2  Elodis; Kronos; 1997; grassroots effort; conta...  \n",
       "3  Elodis; City Council; Kronos; Health Center; C...  \n",
       "4  Henk Bodrogi; Kronos; Protectors of Kronos; SM...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_data = pd.read_csv(\"phi2_mc1_entities_processed.csv\")\n",
    "# Data0 = Data0.rename(columns={0: 'doc_key', 1: 'chunk_index', 2: 'entities', 3: 'latency',\n",
    "#                             4: 'input_tokens',  5: 'output_tokens'})\n",
    "print(entities_data.shape)\n",
    "entities_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b56861-be25-4047-aa8a-6a29504c0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_data.loc[lambda df: df.doc_key == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bffc76a-1131-4f89-9706-5a55c88ecdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json = {}\n",
    "for idx in range(entities_data.shape[0]):\n",
    "    doc_key = entities_data[\"doc_key\"][idx]\n",
    "    chunk_index = entities_data[\"chunk_index\"][idx]\n",
    "    processed_entities = entities_data[\"processed_entities\"][idx]\n",
    "    \n",
    "    if doc_key not in data_json:\n",
    "        data_json[doc_key] = {}\n",
    "        \n",
    "    data_json[doc_key][chunk_index] = processed_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d400201-4346-414e-9158-adef3616f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65119bcc-3daa-4339-95f5-e5f39ec8bc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_json[236])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af1a420-7282-4015-a93c-5366bcc0bea3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aafc774-ace8-4d01-a8bd-fb2cff755784",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa32e25-941b-46e7-b057-bc87d8c90b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/work/pi_dhruveshpate_umass_edu/project_19/ReDocREDPreprocessing/Re-DocRED/processed/\"\n",
    "file_v2 = \"Re-DocRED_Processed_Train.csv\"\n",
    "    \n",
    "def get_entity_example(idx, datatmp):\n",
    "    allEntities = set()\n",
    "    for item in datatmp[\"Triplets\"][idx].split(\"\\n\"):\n",
    "        item = item.split(\" | \")\n",
    "        if len(item) == 3:\n",
    "            allEntities.add(item[0])\n",
    "            allEntities.add(item[2])\n",
    "\n",
    "    return datatmp[\"Text\"][idx], '; '.join(list(allEntities))\n",
    "    \n",
    "def get_entities_prompt(text):\n",
    "    \n",
    "    data_v2 = pd.read_csv(os.path.join(data_path, file_v2), skiprows = range(1, 2000), nrows = 500)\n",
    "    \n",
    "    ex1, exout1 = get_entity_example(1, data_v2)\n",
    "    ex2, exout2 = get_entity_example(2, data_v2)\n",
    "\n",
    "    prompt=f'''Task: Please detect all the entities from the given input Text.\n",
    "Entities could be people, organization, places, concepts, dates or any other proper nouns present in the text. \\\n",
    "Use the following examples as reference to understand the task. \\\n",
    "Give the output in the same format as given in the Example Entities Output, i.e., separated by a semicolon, ';'.\n",
    "\n",
    "Example Text 1: {ex1}\n",
    "Example Entities Output 1: {exout1}\n",
    "\n",
    "Example Text 2: {ex2}\n",
    "Example Entities Output 2: {exout2}\n",
    "\n",
    "Text: {text}\n",
    "Entities Output:'''\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2e4e4-fbba-41c3-aa83-9dc8a219db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = get_entities_prompt(\"text\")\n",
    "len(sample_prompt.split(\" \")), print(sample_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095f4d2a-eba5-4dc4-baf1-07c3bdaa33c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "626c1db7-25a9-4121-a6b8-de38690f9085",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/work/pi_dhruveshpate_umass_edu/project_19/ReDocREDPreprocessing/Re-DocRED/processed/\"\n",
    "file_v2 = \"Re-DocRED_Processed_Train.csv\"\n",
    "    \n",
    "def get_example(idx, datatmp):\n",
    "    allEntities = set()\n",
    "    for item in datatmp[\"Triplets\"][idx].split(\"\\n\"):\n",
    "        item = item.split(\" | \")\n",
    "        # print(item, len(item))\n",
    "        if len(item) == 3:\n",
    "            allEntities.add(item[0])\n",
    "            allEntities.add(item[2])\n",
    "\n",
    "    allEntities = list(allEntities)\n",
    "    # print(allEntities)\n",
    "    # return datatmp[\"Text\"][idx], allEntities, datatmp[\"Triplets\"][idx]\n",
    "    return datatmp[\"Text\"][idx], \"; \".join(allEntities), datatmp[\"Triplets\"][idx]\n",
    "    \n",
    "def get_relations_prompt(text, entities):\n",
    "    \n",
    "    data_v2 = pd.read_csv(os.path.join(data_path, file_v2), skiprows = range(1, 2000), nrows = 500)\n",
    "    \n",
    "    ex1, exent1, exout1 = get_example(1, data_v2)\n",
    "    ex2, exent2, exout2 = get_example(2, data_v2)\n",
    "\n",
    "    prompt=f'''Task Description:\n",
    "The task is to extract Relations between the Entity List for given text, in the form of triplets. \\\n",
    "Extract triplets from the given Text based solely on the relationships present in the text. \\\n",
    "Ensure that entities are chosen directly from the provided Entity List to maintain accuracy. \\\n",
    "Avoid duplicating triplets in the output. Use the provided Example Text and Relations Output as references \\\n",
    "to understand how to identify meaningful relationships between entities from Entity List. \\\n",
    "Pay attention to all potential relations between all the entities and include them in the output.\n",
    "\n",
    "Example Text 1: {ex1}\n",
    "Entity List of Text 1: {exent1}\n",
    "Relations Output of Text 1: {exout1}\n",
    "\n",
    "Example Text 2: {ex2}\n",
    "Entity List of Text 2: {exent2}\n",
    "Relations Output of Text 2: {exout2}\n",
    "\n",
    "Text: {text}\n",
    "Entity List: {entities}\n",
    "Relations Output:'''\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee28a7cc-6e82-4e82-9ec4-874e1bdf6572",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_relations_prompt(\"text\", \"entities\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8bec20-c66a-4656-95c5-7a9835b53ce4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b6e2f01-091f-4264-9d69-32f7812fdff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 256\n",
    "\n",
    "# lower the value, deterministic result\n",
    "temperature = 0.1\n",
    "\n",
    "# a higher value increases the chance of finding a better output\n",
    "top_p = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a8ca26c-66bd-47eb-9747-c417707f5c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name = \"phi2_mc1_triplets_v3.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94b90dfa-db1a-47f1-8eaa-63982545d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "stop_list = [\"Example\", \"Entity\", \"Text:\", \"##\", \"Your task\", \"Entities\", \"Output\"]\n",
    "stop_token_ids = [tokenizer(x,  return_tensors='pt', add_special_tokens=False)['input_ids'].to(\"cuda\") for x in stop_list]\n",
    "\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids[0]):], stop_ids[0]).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7044d13e-dfee-4005-91cd-c6390f3a27b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653001b2-4863-4376-bc02-17cedb767aad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_data = []\n",
    "debug = False\n",
    "\n",
    "all_keys = list(data['Documents'].keys())\n",
    "\n",
    "for i in tqdm(range(len(all_keys))):\n",
    "    \n",
    "    doc_key = all_keys[i]\n",
    "    \n",
    "    for item in data['Documents'][doc_key]:\n",
    "    \n",
    "        #\n",
    "\n",
    "        text = item['chunk_text']\n",
    "        chunk_idx = item['chunk_index']\n",
    "        \n",
    "        # if chunk_idx == 0:\n",
    "        #     continue\n",
    "\n",
    "        # Relation prediction from GT entities\n",
    "        prompt = get_entities_prompt(text)\n",
    "\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\") #,  max_length=4096, truncation=True\n",
    "\n",
    "        start = time.time()\n",
    "        outputs = model.generate(**input_ids,\n",
    "                                 max_new_tokens=max_tokens,\n",
    "                                 top_p=top_p,\n",
    "                                 do_sample=True,\n",
    "                                 temperature=temperature,\n",
    "                                 pad_token_id=tokenizer.eos_token_id,\n",
    "                                 # stopping_criteria=StoppingCriteriaList([stop_criteria]),\n",
    "                                 # stopping_criteria=[Phi2StoppingCriteria()],\n",
    "                                 stopping_criteria=stopping_criteria\n",
    "                                 )\n",
    "        time_diff = time.time() - start\n",
    "\n",
    "        # output2 = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        output2 = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "\n",
    "        if debug:\n",
    "            print(output2)\n",
    "            print(f\"\\nGT : {text}\")\n",
    "\n",
    "        output_data.append([doc_key, chunk_idx, output2, time_diff,\n",
    "                           len(input_ids[0]), len(outputs[0][len(input_ids[0]):])])\n",
    "\n",
    "        if i % 10 == 0 and i > 1:\n",
    "            with open(output_file_name, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerows(output_data)\n",
    "\n",
    "        if debug: break\n",
    "        \n",
    "    if debug: break\n",
    "    \n",
    "    with open(output_file_name, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f306d6-3835-4f36-9759-65c7f4a0b9ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bea346e5-d68b-480a-8c38-a49d61a13f8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Relation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8977e057-a22a-4ad1-959c-294eebcf5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "849734c2-235c-4f2c-bab0-3f228c7066f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 576/847 [1:12:51<1:45:28, 23.35s/it]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "100%|██████████| 847/847 [2:38:12<00:00, 11.21s/it]  \n"
     ]
    }
   ],
   "source": [
    "output_data = []\n",
    "debug = False\n",
    "\n",
    "all_keys = list(data['Documents'].keys())\n",
    "\n",
    "for i in tqdm(range(len(all_keys))):\n",
    "    \n",
    "    doc_key = all_keys[i]\n",
    "    \n",
    "    if int(doc_key) <= 236:\n",
    "        continue\n",
    "    \n",
    "    for item in data['Documents'][doc_key]:\n",
    "    \n",
    "        #\n",
    "\n",
    "        text = item['chunk_text']\n",
    "        chunk_idx = item['chunk_index']\n",
    "        \n",
    "        # if chunk_idx == 0:a\n",
    "        #     continue\n",
    "        \n",
    "        # if int(chunk_index) in data_json[int(doc_key)]:\n",
    "        \n",
    "        processed_entities = data_json[int(doc_key)][chunk_idx]\n",
    "\n",
    "        # Relation prediction from GT entities\n",
    "        if processed_entities:\n",
    "            prompt = get_relations_prompt(text, processed_entities)\n",
    "\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\") #,  max_length=4096, truncation=True\n",
    "\n",
    "            start = time.time()\n",
    "            outputs = model.generate(**input_ids,\n",
    "                                     max_new_tokens=max_tokens,\n",
    "                                     top_p=top_p,\n",
    "                                     do_sample=True,\n",
    "                                     temperature=temperature,\n",
    "                                     pad_token_id=tokenizer.eos_token_id,\n",
    "                                     # stopping_criteria=StoppingCriteriaList([stop_criteria]),\n",
    "                                     # stopping_criteria=[Phi2StoppingCriteria()],\n",
    "                                     stopping_criteria=stopping_criteria\n",
    "                                     )\n",
    "            time_diff = time.time() - start\n",
    "\n",
    "            # output2 = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            output2 = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "\n",
    "            output_data.append([doc_key, chunk_idx, output2, time_diff,\n",
    "                           len(input_ids[0]), len(outputs[0][len(input_ids[0]):])])\n",
    "\n",
    "        else:\n",
    "            output2 = \"No entities detected\"\n",
    "\n",
    "            output_data.append([doc_key, chunk_idx, output2, time_diff,\n",
    "                           0, 0])\n",
    "                \n",
    "#         else:\n",
    "#                 output2 = \"No data\"\n",
    "\n",
    "#                 output_data.append([doc_key, chunk_idx, output2, time_diff,\n",
    "#                                0, 0])\n",
    "\n",
    "        if debug:\n",
    "            print(output2)\n",
    "            # print(f\"\\nGT : {text}\")\n",
    "\n",
    "        \n",
    "\n",
    "        if i % 10 == 0 and i > 1:\n",
    "            with open(output_file_name, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerows(output_data)\n",
    "\n",
    "        if debug: break\n",
    "        \n",
    "    if debug: break\n",
    "    \n",
    "    with open(output_file_name, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b0dc5-b94a-4954-a677-750a064248d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_key, chunk_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3819120e-a58c-418f-8ae7-b47a6786a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[int(doc_key)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1cb704-a84c-445b-9afb-e87df811b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e2af2-bac5-4ed8-b077-03227470fe7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Process Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ffd74e-4c18-48c5-9c76-ae3fd0727942",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data0 = pd.read_csv(\"phi2_mc1_entities.csv\", header=None)\n",
    "Data0 = Data0.rename(columns={0: 'doc_key', 1: 'chunk_index', 2: 'entities', 3: 'latency',\n",
    "                            4: 'input_tokens',  5: 'output_tokens'})\n",
    "Data0[\"processed_entities\"] = \"\"\n",
    "Data0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09696ca3-7276-4389-b011-6389c408cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 184, 202, 224, 261, 265, 269\n",
    "Data0[\"entities\"][34], \"\".split(\"; \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab011806-0879-486c-a3e3-6f7795977a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i in range(Data0.shape[0]):\n",
    "    \n",
    "    # if i != 261: # 1, 5, 27\n",
    "    #     continue\n",
    "        \n",
    "    ent = Data0[\"entities\"][i]\n",
    "    ent = re.sub(r\"\\n\\d+. \", \"; \", ent)\n",
    "    ent = ent.strip()\n",
    "    # print(len(ent.split(\"\\n\")))\n",
    "    \n",
    "    if \"- \" in ent:\n",
    "        # ent2 = re.sub(\"\\s+\", \"\", ent2)\n",
    "        ent2 = ent.replace(\"\\n\", \"\")\n",
    "        ent2 = ent2.replace(\"- \", \"; \")\n",
    "        ent2 = ent2.split(\"; \")\n",
    "    \n",
    "    elif len(ent.split(\"\\n\")) > 1:\n",
    "        ent2 = ent.split(\"\\n\")[0].split(\"; \")\n",
    "        \n",
    "    elif \";\" not in ent:\n",
    "        ent2 = \"\".split(\"; \")\n",
    "    \n",
    "    else:\n",
    "        ent2 = ent.split(\"; \")\n",
    "        \n",
    "    ent3 = []\n",
    "    for item in ent2:\n",
    "        item = item.strip()\n",
    "        if item and item not in ent3:\n",
    "            ent3.append(item)\n",
    "    ent3 = \"; \".join(ent3)\n",
    "    \n",
    "    # print(f\"\\n{i}\")\n",
    "    # print(ent)\n",
    "    # print(f\"\\nProcessed entities - {ent3}\")\n",
    "    # print(\"*\"*20)\n",
    "    Data0[\"processed_entities\"][i] = ent3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f8c03f-c41c-431c-bfdc-283041f1a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data0[\"processed_entities\"][224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1decbd3d-e55e-4dfc-9ced-591cc4074d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data0.head()\n",
    "Data0.to_csv('phi2_mc1_entities_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd11dc-0b10-476a-b4fc-760362c93916",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent.split(\"\\n\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e180db-4c67-4242-8956-1eac3791b8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5801dd-08d0-462a-a809-399499dfefaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1f87a24-85f7-4170-bbaf-3a99cdb89712",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Combine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c61a996-923c-40ae-aa18-927ecdc66978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1339, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_key</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>entities</th>\n",
       "      <th>latency</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>University of Abila | sponsor | One World Res...</td>\n",
       "      <td>4.131064</td>\n",
       "      <td>1330</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Sri Lanka | country | 30 June 1972\\nSri Lanka...</td>\n",
       "      <td>7.259529</td>\n",
       "      <td>1359</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Elodis | capital city | Kronos\\nElodis | inco...</td>\n",
       "      <td>7.212500</td>\n",
       "      <td>1306</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Elodis | concerned citizens | Elodis Citizens...</td>\n",
       "      <td>7.236583</td>\n",
       "      <td>1309</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Kronos | community | Protectors of Kronos\\nKr...</td>\n",
       "      <td>6.532060</td>\n",
       "      <td>1322</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_key  chunk_index                                           entities  \\\n",
       "0        0            0   University of Abila | sponsor | One World Res...   \n",
       "1        0            1   Sri Lanka | country | 30 June 1972\\nSri Lanka...   \n",
       "2        0            2   Elodis | capital city | Kronos\\nElodis | inco...   \n",
       "3        0            3   Elodis | concerned citizens | Elodis Citizens...   \n",
       "4        0            4   Kronos | community | Protectors of Kronos\\nKr...   \n",
       "\n",
       "    latency  input_tokens  output_tokens  \n",
       "0  4.131064          1330            141  \n",
       "1  7.259529          1359            256  \n",
       "2  7.212500          1306            256  \n",
       "3  7.236583          1309            256  \n",
       "4  6.532060          1322            231  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homeFolder = \"/work/pi_dhruveshpate_umass_edu/project_19/aishwarya/696DS-named-entity-extraction-and-linking-for-KG-construction/code/phi2\"\n",
    "\n",
    "DataPath0 = os.path.join(homeFolder, \"phi2_mc1_triplets_v2.csv\")\n",
    "DataPath1 = os.path.join(homeFolder, \"phi2_mc1_triplets_v3.csv\")\n",
    "# DataPath2 = os.path.join(homeFolder, \"phi2_relations_gt_redocred_dev_v3.csv\")\n",
    "# DataPath3 = os.path.join(homeFolder, \"phi2_relations_gt_redocred_dev_v4.csv\")\n",
    "\n",
    "Data0 = pd.read_csv(DataPath0, header=None)\n",
    "Data1 = pd.read_csv(DataPath1, header=None)\n",
    "# Data2 = pd.read_csv(DataPath2, header=None)\n",
    "# Data3 = pd.read_csv(DataPath3, header=None)\n",
    "Data = pd.concat([Data0, Data1], axis=0)\n",
    "\n",
    "Data = Data.rename(columns={0: 'doc_key', 1: 'chunk_index', 2: 'entities', 3: 'latency',\n",
    "                            4: 'input_tokens',  5: 'output_tokens'})\n",
    "\n",
    "Data.to_csv('phi2_mc1_triplets_combined.csv', index=False)\n",
    "\n",
    "# print(Data.shape, len(list(set(list(Data[\"original_index\"])))))\n",
    "print(Data.shape)\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24afc088-bb3a-47e0-a8fc-4cb4fbdc2780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b2fa43-de6d-44e9-9154-9c4b9119f201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-hw1]",
   "language": "python",
   "name": "conda-env-.conda-hw1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
