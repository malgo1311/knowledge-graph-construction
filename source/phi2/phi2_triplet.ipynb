{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ed86e9-5da5-43aa-b0dd-65426d877cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_DATASETS_CACHE'] = '/work/pi_dhruveshpate_umass_edu/amalgonde_umass_edu'\n",
    "os.environ['CONDA_PKGS_DIRS'] = '/work/pi_dhruveshpate_umass_edu/amalgonde_umass_edu'\n",
    "os.environ['CONDA_ENVS_PATH'] = '/work/pi_dhruveshpate_umass_edu/amalgonde_umass_edu'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/work/pi_dhruveshpate_umass_edu/amalgonde_umass_edu'\n",
    "os.environ['HF_HOME'] = '/work/pi_dhruveshpate_umass_edu/amalgonde_umass_edu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3df1e92a-df79-45c2-9a32-c895e232346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amalgonde_umass_edu/.conda/envs/hw1/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n",
      "- configuration_phi.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n",
      "- modeling_phi.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f468355cef4d338fc5c7cf1163eaeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425cb97ed383412c8eb5a3c36fc42864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             trust_remote_code=True,\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\",\n",
    "                                             # load_in_8bit=True\n",
    "                                            )\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef0502-32b4-454b-83c4-c426c60d2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, padding='max_length').to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=1024)\n",
    "print(type(tokenizer.decode(outputs[0])), tokenizer.decode(outputs[0])) #.replace('<pad>', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b307017a-1c8a-42cb-8d1f-90aa4d147104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['original_index', 'category', 'eid', 'size', 'original_triple_sets',\n",
       "       'modified_triple_sets', 'comment', 'lid', 'text', 'lang', 'shape',\n",
       "       'shape_type', 'dbpedia_links', 'links', 'test_category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_path = \"/work/pi_dhruveshpate_umass_edu/project_19/aishwarya/696DS-named-entity-extraction-and-linking-for-KG-construction/webnlg/data/release_v3.0_en/sample_dev_data/\"\n",
    "file = \"webnlg_dev_sample.csv\"\n",
    "\n",
    "data = pd.read_csv(os.path.join(data_path, file))\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2498b82f-ea7e-40a4-a2fd-e94b52b956ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import get_prompts_exp2 as get_prompts\n",
    "\n",
    "import json\n",
    "\n",
    "with open('/work/pi_dhruveshpate_umass_edu/project_19/aishwarya//696DS-named-entity-extraction-and-linking-for-KG-construction/webnlg/webNLG_dev_relations.json', 'r') as fp:\n",
    "    relationList = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b6e2f01-091f-4264-9d69-32f7812fdff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 256\n",
    "\n",
    "# lower the value, deterministic result\n",
    "temperature = 0.1\n",
    "\n",
    "# a higher value increases the chance of finding a better output\n",
    "top_p = 0.9\n",
    "\n",
    "output_file_name = \"phi2_fewdhot_webnlg_dev_sample.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03d6b443-d494-405d-87aa-60ee16b3e2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14402, 14402, 14402]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class KeywordsStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, keywords_ids:list):\n",
    "        self.keywords = keywords_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        if input_ids[0][-1] in self.keywords:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "stop_words = ['Test', 'Test Sentence', 'Test Output']\n",
    "stop_ids = [tokenizer.encode(w)[0] for w in stop_words]\n",
    "stop_criteria = KeywordsStoppingCriteria(stop_ids)\n",
    "stop_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "653001b2-4863-4376-bc02-17cedb767aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 878/878 [1:00:14<00:00,  4.12s/it]\n"
     ]
    }
   ],
   "source": [
    "all_inputs = list(data['text'])\n",
    "output_data = []\n",
    "debug = False\n",
    "\n",
    "for i in tqdm(range(data.shape[0])):\n",
    "    \n",
    "    # if i <= 1940:\n",
    "    #     continue\n",
    "    \n",
    "    # prompt = zero_shot_prompt(all_inputs[i])\n",
    "    # prompt = get_prompts.one_shot_prompt_v2(all_inputs[i], relationList['all_relations'])\n",
    "    prompt = get_prompts.few_shot_prompt_v1(all_inputs[i])\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    start = time.time()\n",
    "    outputs = model.generate(**input_ids,\n",
    "                             max_new_tokens=max_tokens,\n",
    "                             top_p=top_p,\n",
    "                             do_sample=True,\n",
    "                             temperature=temperature,\n",
    "                             pad_token_id=tokenizer.eos_token_id,\n",
    "                             stopping_criteria=StoppingCriteriaList([stop_criteria]),\n",
    "                             )\n",
    "    time_diff = time.time() - start\n",
    "    # inferenceTime.append()\n",
    "    \n",
    "    # output2 = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    output2 = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "    \n",
    "    if debug:\n",
    "        print(output2)\n",
    "        print(f\"\\nGT : {data['modified_triple_sets'][i]}\")\n",
    "        \n",
    "    output_data.append([data[\"original_index\"][i], output2, time_diff])\n",
    "    \n",
    "    if i % 10 == 0 and i > 1:\n",
    "        with open(output_file_name, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(output_data)\n",
    "    # break\n",
    "with open(output_file_name, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b849ed0-a3b6-4743-bb68-ac42c9dda7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tesla V100-PCIE-16GB - phi2_dev\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-hw1]",
   "language": "python",
   "name": "conda-env-.conda-hw1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
