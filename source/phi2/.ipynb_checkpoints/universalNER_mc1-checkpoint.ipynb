{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc3aa3bb-e438-4245-8d36-101d5a587ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21a8e5ad-4d95-4411-b4ee-fddd185d905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_CACHE'] = '/work/pi_dhruveshpate_umass_edu/abaranwal_umass_edu'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/work/pi_dhruveshpate_umass_edu/abaranwal_umass_edu'\n",
    "os.environ['CONDA_ENVS_PATH'] = '/work/pi_dhruveshpate_umass_edu/abaranwal_umass_edu'\n",
    "os.environ['CONDA_PKGS_DIRS'] = '/work/pi_dhruveshpate_umass_edu/abaranwal_umass_edu'\n",
    "os.environ['HF_HOME'] = '/work/pi_dhruveshpate_umass_edu/abaranwal_umass_edu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f6386-fc84-4c00-b424-e05b67834e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df1e92a-df79-45c2-9a32-c895e232346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Universal-NER/UniNER-7B-all\"\n",
    "# model_name = \"google/gemma-2b\"\n",
    "\n",
    "# config = AutoConfig.from_pretrained(model_name)\n",
    "# config.max_seq_len = 4096\n",
    "# config.max_answer_len= 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"Model max length - {tokenizer.model_max_length}\")\n",
    "# tokenizer.model_max_length = 4096\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             # max_length=4096,\n",
    "                                             trust_remote_code=True,\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\",\n",
    "                                             # load_in_8bit=True\n",
    "                                            )\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623e9604-1070-4e0a-a116-6ce0196fdf3e",
   "metadata": {},
   "source": [
    "## Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1f4dab-2ed9-4dec-b0a8-0c5a4d66c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/work/pi_dhruveshpate_umass_edu/project_19/astha/696DS-named-entity-extraction-and-linking-for-KG-construction/code/mc1/mc1_preprocess/\"\n",
    "file = \"mc1_chunked_data.json\"\n",
    "\n",
    "with open(os.path.join(data_path, file), 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c09b28e-f48a-4980-9ce8-689ef36adf55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2af1a420-7282-4015-a93c-5366bcc0bea3",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa32e25-941b-46e7-b057-bc87d8c90b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/work/pi_dhruveshpate_umass_edu/project_19/ReDocREDPreprocessing/Re-DocRED/processed/\"\n",
    "file_v2 = \"Re-DocRED_Processed_Train.csv\"\n",
    "    \n",
    "def get_entity_example(idx, datatmp):\n",
    "    allEntities = set()\n",
    "    for item in datatmp[\"Triplets\"][idx].split(\"\\n\"):\n",
    "        item = item.split(\" | \")\n",
    "        if len(item) == 3:\n",
    "            allEntities.add(item[0])\n",
    "            allEntities.add(item[2])\n",
    "\n",
    "    return datatmp[\"Text\"][idx], '; '.join(list(allEntities))\n",
    "    \n",
    "def get_entities_prompt(text):\n",
    "    \n",
    "    data_v2 = pd.read_csv(os.path.join(data_path, file_v2), skiprows = range(1, 2000), nrows = 500)\n",
    "    \n",
    "    ex1, exout1 = get_entity_example(1, data_v2)\n",
    "    ex2, exout2 = get_entity_example(2, data_v2)\n",
    "\n",
    "    prompt=f'''Task: Please detect all the entities from the given input Text.\n",
    "Entities could be people, organization, places, concepts, dates or any other proper nouns present in the text. \\\n",
    "Use the following examples as reference to understand the task. \\\n",
    "Give the output in the same format as given in the Example Entities Output, i.e., separated by a semicolon, ';'.\n",
    "\n",
    "Example Text 1: {ex1}\n",
    "Example Entities Output 1: {exout1}\n",
    "\n",
    "Example Text 2: {ex2}\n",
    "Example Entities Output 2: {exout2}\n",
    "\n",
    "Text: {text}\n",
    "Entities Output:'''\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def few_shot_prompt_universalNER(entity_type, input_text):\n",
    "    prompt = f\"\"\"\n",
    "Given a Text, your task is to extract all entities based on the given Entity type.\n",
    "\n",
    "Text: {input_text}\n",
    "\n",
    "Entity Type: {entity_type}\n",
    "\n",
    "Output: \"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def few_shot_prompt_universalNER_v2(entity_type, input_text):\n",
    "    prompt = f\"\"\"Text: {input_text}\n",
    "\n",
    "What describes {entity_type} in the text?\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def few_shot_prompt_universalNER_v3(entity_type, input_text):\n",
    "    prompt = {\"user\": f\"Text: {input_text}\",\n",
    "             \"assistant\": \"I've read this text.\",\n",
    "             \"user\": f\"What describes {entity_type} in the text?\"}\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2e4e4-fbba-41c3-aa83-9dc8a219db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = get_entities_prompt(\"text\")\n",
    "len(sample_prompt.split(\" \")), print(sample_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8bec20-c66a-4656-95c5-7a9835b53ce4",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6e2f01-091f-4264-9d69-32f7812fdff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 256\n",
    "\n",
    "# lower the value, deterministic result\n",
    "temperature = 0.1\n",
    "\n",
    "# a higher value increases the chance of finding a better output\n",
    "top_p = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8ca26c-66bd-47eb-9747-c417707f5c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name = \"universalner_mc1_entities.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b90dfa-db1a-47f1-8eaa-63982545d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "stop_list = [\"Example\", \"Entity\", \"Text:\", \"##Your task\", \"Entities\", \"Output\", \"entity\"]\n",
    "stop_token_ids = [tokenizer(x,  return_tensors='pt', add_special_tokens=False)['input_ids'].to(\"cuda\") for x in stop_list]\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids[0]):], stop_ids[0]).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653001b2-4863-4376-bc02-17cedb767aad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_data = []\n",
    "debug = True\n",
    "\n",
    "# entity_types = [\"person\", \"organization\", \"location\", \"dates\", \"number\", \"product\", \"event\", \"language\", \"group\", \"misc\"]\n",
    "entity_types = [\"person\", \"organization\", \"location\", \"datetime\", \"concept\", \"event\", \"group\"]\n",
    "\n",
    "all_keys = list(data['Documents'].keys())\n",
    "\n",
    "for i in tqdm(range(len(all_keys))):\n",
    "    \n",
    "    doc_key = all_keys[i]\n",
    "    \n",
    "    if i <= 1:\n",
    "        continue\n",
    "    \n",
    "    for item in data['Documents'][doc_key]:\n",
    "    \n",
    "        text = item['chunk_text']\n",
    "        chunk_idx = item['chunk_index']\n",
    "        \n",
    "        # if chunk_idx == 0:\n",
    "        #     continue\n",
    "        \n",
    "        tmp_output = {}\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\ntext : {text}\")\n",
    "        \n",
    "        for ent_typ in entity_types:\n",
    "\n",
    "            # Relation prediction from GT entities\n",
    "            prompt = few_shot_prompt_universalNER_v2(ent_typ, text)\n",
    "            \n",
    "            # if debug:\n",
    "            #     print(f\"\\nprompt : {prompt}\")\n",
    "\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\") #,  max_length=4096, truncation=True\n",
    "\n",
    "            start = time.time()\n",
    "            outputs = model.generate(**input_ids,\n",
    "                                     max_new_tokens=max_tokens,\n",
    "                                     top_p=top_p,\n",
    "                                     do_sample=True,\n",
    "                                     temperature=temperature,\n",
    "                                     pad_token_id=tokenizer.eos_token_id,\n",
    "                                     # stopping_criteria=StoppingCriteriaList([stop_criteria]),\n",
    "                                     # stopping_criteria=[Phi2StoppingCriteria()],\n",
    "                                     stopping_criteria=stopping_criteria\n",
    "                                     )\n",
    "            time_diff = time.time() - start\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"{ent_typ} : {output2}\")\n",
    "\n",
    "            # output2 = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            output2 = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "            \n",
    "            tmp_output[ent_typ] = {\"output\": output2, \"time_diff\": time_diff,\n",
    "                                   \"input_tokens\": len(input_ids[0]),\n",
    "                                   \"output_tokens\":  len(outputs[0][len(input_ids[0]):])}\n",
    "\n",
    "        # if debug:\n",
    "        #     print(f\"Output : {tmp_output}\")\n",
    "\n",
    "        # output_data.append([doc_key, chunk_idx, output2, time_diff,\n",
    "        #                    len(input_ids[0]), len(outputs[0][len(input_ids[0]):])])\n",
    "        output_data.append([doc_key, chunk_idx, tmp_output])\n",
    "\n",
    "        if i % 10 == 0 and i > 1:\n",
    "            with open(output_file_name, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerows(output_data)\n",
    "\n",
    "        if debug: break\n",
    "        \n",
    "    if debug: break\n",
    "    \n",
    "    with open(output_file_name, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849734c2-235c-4f2c-bab0-3f228c7066f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f87a24-85f7-4170-bbaf-3a99cdb89712",
   "metadata": {},
   "source": [
    "## Combine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c61a996-923c-40ae-aa18-927ecdc66978",
   "metadata": {},
   "outputs": [],
   "source": [
    "homeFolder = \"/work/pi_dhruveshpate_umass_edu/project_19/aishwarya/696DS-named-entity-extraction-and-linking-for-KG-construction/code/phi2\"\n",
    "\n",
    "DataPath0 = os.path.join(homeFolder, \"phi2_relations_gt_redocred_dev.csv\")\n",
    "DataPath1 = os.path.join(homeFolder, \"phi2_relations_gt_redocred_dev_v2.csv\")\n",
    "DataPath2 = os.path.join(homeFolder, \"phi2_relations_gt_redocred_dev_v3.csv\")\n",
    "DataPath3 = os.path.join(homeFolder, \"phi2_relations_gt_redocred_dev_v4.csv\")\n",
    "\n",
    "Data0 = pd.read_csv(DataPath0, header=None)\n",
    "Data1 = pd.read_csv(DataPath1, header=None)\n",
    "Data2 = pd.read_csv(DataPath2, header=None)\n",
    "Data3 = pd.read_csv(DataPath3, header=None)\n",
    "Data = pd.concat([Data0, Data1, Data2, Data3], axis=0)\n",
    "\n",
    "Data = Data.rename(columns={0: 'original_index', 1: 'entities', 2: 'latency',\n",
    "                            3: 'input_tokens',  4: 'output_tokens'})\n",
    "\n",
    "Data.to_csv('phi2_relations_gt_redocred_dev_combined.csv', index=False)\n",
    "\n",
    "print(Data.shape, len(list(set(list(Data[\"original_index\"])))))\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24afc088-bb3a-47e0-a8fc-4cb4fbdc2780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b2fa43-de6d-44e9-9154-9c4b9119f201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-hw1]",
   "language": "python",
   "name": "conda-env-.conda-hw1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
