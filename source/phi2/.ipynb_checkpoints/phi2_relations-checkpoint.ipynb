{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04afdb07-3e5d-460f-968f-0be2e71fab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69ed86e9-5da5-43aa-b0dd-65426d877cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_DATASETS_CACHE'] = '/work/pi_dhruveshpate_umass_edu/amalgonde_umass_edu'\n",
    "os.environ['CONDA_PKGS_DIRS'] = '/work/pi_dhruveshpate_umass_edu/amalgonde_umass_edu'\n",
    "os.environ['CONDA_ENVS_PATH'] = '/work/pi_dhruveshpate_umass_edu/amalgonde_umass_edu'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/work/pi_dhruveshpate_umass_edu/amalgonde_umass_edu'\n",
    "os.environ['HF_HOME'] = '/work/pi_dhruveshpate_umass_edu/amalgonde_umass_edu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3df1e92a-df79-45c2-9a32-c895e232346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amalgonde_umass_edu/.conda/envs/hw1/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n",
      "- configuration_phi.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model max length - 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n",
      "- modeling_phi.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2d73c1bcc54680a7da2d368b4d0daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a6ce74997d46f9bc2ab1f019fbad49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "# model_name = \"google/gemma-2b\"\n",
    "\n",
    "# config = AutoConfig.from_pretrained(model_name)\n",
    "# config.max_seq_len = 4096\n",
    "# config.max_answer_len= 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"Model max length - {tokenizer.model_max_length}\")\n",
    "# tokenizer.model_max_length = 4096\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             # max_length=4096,\n",
    "                                             trust_remote_code=True,\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\",\n",
    "                                             # load_in_8bit=True\n",
    "                                            )\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef0502-32b4-454b-83c4-c426c60d2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST\n",
    "\n",
    "# input_text = \"Write me a poem about Machine Learning.\"\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "# # input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, padding='max_length').to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(**input_ids, max_new_tokens=1024)\n",
    "# print(type(tokenizer.decode(outputs[0])), tokenizer.decode(outputs[0])) #.replace('<pad>', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623e9604-1070-4e0a-a116-6ce0196fdf3e",
   "metadata": {},
   "source": [
    "## Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a1f4dab-2ed9-4dec-b0a8-0c5a4d66c8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Triplets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Willi Schneider (skeleton racer)</td>\n",
       "      <td>Wilfried \" Willi \" Schneider (born 13 March 19...</td>\n",
       "      <td>2002 Winter Olympics | start time | 2002\\n2002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ross Alger</td>\n",
       "      <td>Ross Patterson Alger (August 20, 1920  January...</td>\n",
       "      <td>Ross Patterson Alger | place of birth | Prelat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Mess of Blues (Jeff Healey album)</td>\n",
       "      <td>Mess of Blues is an album by Jeff Healey. It w...</td>\n",
       "      <td>Mess of Blues | publication date | 2008\\nMess ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ramey Idriss</td>\n",
       "      <td>Ramey Idriss (11 September 1911  5 February 19...</td>\n",
       "      <td>Wet Blanket Policy | publication date | 1948\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ELAM (Latin American School of Medicine) Cuba</td>\n",
       "      <td>Escuela Latinoamericana de Medicina (ELAM), fo...</td>\n",
       "      <td>Latin American School of Medicine | country | ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_index                                          Title  \\\n",
       "0               0               Willi Schneider (skeleton racer)   \n",
       "1               1                                     Ross Alger   \n",
       "2               2              Mess of Blues (Jeff Healey album)   \n",
       "3               3                                   Ramey Idriss   \n",
       "4               4  ELAM (Latin American School of Medicine) Cuba   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Wilfried \" Willi \" Schneider (born 13 March 19...   \n",
       "1  Ross Patterson Alger (August 20, 1920  January...   \n",
       "2  Mess of Blues is an album by Jeff Healey. It w...   \n",
       "3  Ramey Idriss (11 September 1911  5 February 19...   \n",
       "4  Escuela Latinoamericana de Medicina (ELAM), fo...   \n",
       "\n",
       "                                            Triplets  \n",
       "0  2002 Winter Olympics | start time | 2002\\n2002...  \n",
       "1  Ross Patterson Alger | place of birth | Prelat...  \n",
       "2  Mess of Blues | publication date | 2008\\nMess ...  \n",
       "3  Wet Blanket Policy | publication date | 1948\\n...  \n",
       "4  Latin American School of Medicine | country | ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/work/pi_dhruveshpate_umass_edu/project_19/ReDocREDPreprocessing/Re-DocRED/processed/\"\n",
    "file = \"Re-DocRED_Processed_Dev.csv\"\n",
    "\n",
    "data = pd.read_csv(os.path.join(data_path, file))\n",
    "data = data.rename(columns={'Unnamed: 0': 'original_index', 'index': 'original_index'})\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd44a8e8-c119-4d7e-8d8f-e7c387e9a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "entityPredictions = \"/work/pi_dhruveshpate_umass_edu/project_19/aishwarya/696DS-named-entity-extraction-and-linking-for-KG-construction/code/llama2/lamma2_entity_redocred_dev.csv\"\n",
    "\n",
    "dataEntity = pd.read_csv(entityPredictions, header=None)\n",
    "dataEntity = dataEntity.rename(columns={0: 'original_index', 1: \"Entities\", 2: \"InferenceTime\"})\n",
    "print(dataEntity.shape)\n",
    "dataEntity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f9ec3-c0fa-45c8-a62e-12f587084118",
   "metadata": {},
   "outputs": [],
   "source": [
    "completeData = pd.merge(data, dataEntity, on='original_index')\n",
    "print(completeData.shape)\n",
    "completeData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c09b28e-f48a-4980-9ce8-689ef36adf55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2af1a420-7282-4015-a93c-5366bcc0bea3",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa32e25-941b-46e7-b057-bc87d8c90b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/work/pi_dhruveshpate_umass_edu/project_19/ReDocREDPreprocessing/Re-DocRED/processed/\"\n",
    "file_v2 = \"Re-DocRED_Processed_Train.csv\"\n",
    "    \n",
    "def get_example(idx, datatmp):\n",
    "    allEntities = set()\n",
    "    for item in datatmp[\"Triplets\"][idx].split(\"\\n\"):\n",
    "        item = item.split(\" | \")\n",
    "        # print(item, len(item))\n",
    "        if len(item) == 3:\n",
    "            allEntities.add(item[0])\n",
    "            allEntities.add(item[2])\n",
    "\n",
    "    allEntities = list(allEntities)\n",
    "    # print(allEntities)\n",
    "    return datatmp[\"Text\"][idx], allEntities, datatmp[\"Triplets\"][idx]\n",
    "    # return datatmp[\"Text\"][idx], \"; \".join(allEntities), datatmp[\"Triplets\"][idx]\n",
    "    \n",
    "def get_prompt(text, entities):\n",
    "    \n",
    "    data_v2 = pd.read_csv(os.path.join(data_path, file_v2), skiprows = range(1, 2000), nrows = 500)\n",
    "    \n",
    "    ex1, exent1, exout1 = get_example(100, data_v2)\n",
    "    ex2, exent2, exout2 = get_example(200, data_v2)\n",
    "    # ex3, exent3, exout3 = get_example(300, data_v2)\n",
    "    # ex4, exent4, exout4 = get_example(400, data_v2)\n",
    "\n",
    "    prompt=f'''Task Description:\n",
    "The task is to extract Relations between the Entity List for given text, in the form of triplets. \\\n",
    "Extract triplets from the given Text based solely on the relationships present in the text. \\\n",
    "Ensure that entities are chosen directly from the provided Entity List to maintain accuracy. \\\n",
    "Avoid duplicating triplets in the output. Use the provided Example Text and Relations Output as references \\\n",
    "to understand how to identify meaningful relationships between entities from Entity List. \\\n",
    "Pay attention to all potential relations between all the entities and include them in the output.\n",
    "\n",
    "Example Text 1: {ex1}\n",
    "Entity List of Text 1: {exent1}\n",
    "Relations Output of Text 1: {exout1}\n",
    "\n",
    "Example Text 2: {ex2}\n",
    "Entity List of Text 2: {exent2}\n",
    "Relations Output of Text 2: {exout2}\n",
    "\n",
    "Text: {text}\n",
    "Entity List: {entities}\n",
    "Relations Output:'''\n",
    "    \n",
    "#     Example Text 3: {ex3}\n",
    "#     Entity List of Text 3: {exent3}\n",
    "#     Relations Output of Text 3: {exout3}\n",
    "\n",
    "#     Example Text 4: {ex4}\n",
    "#     Entity List of Text 4: {exent4}\n",
    "#     Relations Output of Text 4: {exout4}\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2e4e4-fbba-41c3-aa83-9dc8a219db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = get_prompt(\"text\", \"abd; abc\")\n",
    "len(sample_prompt.split(\" \")) #, sample_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8bec20-c66a-4656-95c5-7a9835b53ce4",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b6e2f01-091f-4264-9d69-32f7812fdff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 512\n",
    "\n",
    "# lower the value, deterministic result\n",
    "temperature = 0.1\n",
    "\n",
    "# a higher value increases the chance of finding a better output\n",
    "top_p = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03d6b443-d494-405d-87aa-60ee16b3e2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16281, 32398, 47117]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class KeywordsStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, keywords_ids:list):\n",
    "        self.keywords = keywords_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        if input_ids[0][-1] in self.keywords:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "# stop_words = ['Test', 'Test Sentence', 'Test Output']\n",
    "stop_words = [\"Example\", \"Entity\", \"Relations\"]\n",
    "stop_ids = [tokenizer.encode(w)[0] for w in stop_words]\n",
    "stop_criteria = KeywordsStoppingCriteria(stop_ids)\n",
    "stop_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91e2faf8-5c56-427b-ba3e-28b60df148a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_entities_v1(ent):\n",
    "    ent = ent.strip()\n",
    "    ent = ent.split(\"; \")\n",
    "    ent = list(set(ent))\n",
    "    ent = \"; \".join(ent)\n",
    "    return ent\n",
    "\n",
    "def process_gtEntities(triplets):\n",
    "    \n",
    "    if not isinstance(triplets,str):\n",
    "        print(f\"Not string - {triplets}\")\n",
    "            \n",
    "    triplets = triplets.strip()\n",
    "    triplets = triplets.split(\"\\n\")\n",
    "    \n",
    "    output = set()\n",
    "    for t in triplets:\n",
    "        if not isinstance(t,str):\n",
    "            print(f\"Not string - {t}\")\n",
    "            continue\n",
    "            \n",
    "        t = t.split(\" | \")\n",
    "        if len(t) != 3:\n",
    "            continue\n",
    "            \n",
    "        if len(t) != 3:\n",
    "            print(f\"Not len 3 - {t}\")\n",
    "            continue\n",
    "            \n",
    "        output.add(t[0])\n",
    "        output.add(t[2])\n",
    "        \n",
    "    output = list(output)\n",
    "    return output\n",
    "\n",
    "# enty = completeData[\"Entities\"][2]\n",
    "# enty, process_entities(enty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a8ca26c-66bd-47eb-9747-c417707f5c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name = \"phi2_relations_gt_redocred_dev_v4.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "653001b2-4863-4376-bc02-17cedb767aad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 495/500 [00:44<00:00,  7.56it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2147 > 2048). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 500/500 [01:58<00:00,  4.22it/s]\n"
     ]
    }
   ],
   "source": [
    "output_data = []\n",
    "debug = False\n",
    "\n",
    "for i in tqdm(range(data.shape[0])):\n",
    "    \n",
    "    if i <= 40 and debug:\n",
    "        continue\n",
    "        \n",
    "    if i < 491 or i == 492:\n",
    "        continue\n",
    "        \n",
    "    gt = data['Triplets'][i]\n",
    "    ent = process_gtEntities(data[\"Triplets\"][i])\n",
    "    \n",
    "    # Two step prediction\n",
    "    # prompt = get_prompt(completeData[\"Text\"][i], process_entities_v1(completeData[\"Entities\"][i]))\n",
    "    # gt = completeData['Triplets'][i]\n",
    "    # ent = completeData['Entities'][i]\n",
    "    \n",
    "    # Relation prediction from GT entities\n",
    "    prompt = get_prompt(data[\"Text\"][i], ent)\n",
    "    \n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\") #,  max_length=4096, truncation=True\n",
    "    \n",
    "    start = time.time()\n",
    "    outputs = model.generate(**input_ids,\n",
    "                             max_new_tokens=max_tokens,\n",
    "                             top_p=top_p,\n",
    "                             do_sample=True,\n",
    "                             temperature=temperature,\n",
    "                             pad_token_id=tokenizer.eos_token_id,\n",
    "                             stopping_criteria=StoppingCriteriaList([stop_criteria]),\n",
    "                             )\n",
    "    time_diff = time.time() - start\n",
    "    \n",
    "    # output2 = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    output2 = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "    \n",
    "    if debug:\n",
    "        print(output2)\n",
    "        print(f\"\\nGT : {gt}\")\n",
    "        print(f\"\\nEntities : {ent}\")\n",
    "        \n",
    "    output_data.append([data[\"original_index\"][i], output2, time_diff,\n",
    "                       len(input_ids[0]), len(outputs[0][len(input_ids[0]):])])\n",
    "    \n",
    "    if i % 10 == 0 and i > 1:\n",
    "        with open(output_file_name, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(output_data)\n",
    "            \n",
    "    if debug: break\n",
    "    \n",
    "with open(output_file_name, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849734c2-235c-4f2c-bab0-3f228c7066f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1f87a24-85f7-4170-bbaf-3a99cdb89712",
   "metadata": {},
   "source": [
    "## Combine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c61a996-923c-40ae-aa18-927ecdc66978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(499, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_index</th>\n",
       "      <th>entities</th>\n",
       "      <th>latency</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Transylvania | location | Germany\\nVancouver ...</td>\n",
       "      <td>8.300891</td>\n",
       "      <td>1795</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ross Patterson Alger | born in Prelate, Saska...</td>\n",
       "      <td>20.916132</td>\n",
       "      <td>1829</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Jeff Healey | released | Mess of Blues\\nJeff ...</td>\n",
       "      <td>20.942740</td>\n",
       "      <td>1714</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>My Three Sons | television series | George Ti...</td>\n",
       "      <td>10.806677</td>\n",
       "      <td>1865</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Cuba | located in the administrative territor...</td>\n",
       "      <td>21.112151</td>\n",
       "      <td>1859</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_index                                           entities  \\\n",
       "0               0   Transylvania | location | Germany\\nVancouver ...   \n",
       "1               1   Ross Patterson Alger | born in Prelate, Saska...   \n",
       "2               2   Jeff Healey | released | Mess of Blues\\nJeff ...   \n",
       "3               3   My Three Sons | television series | George Ti...   \n",
       "4               4   Cuba | located in the administrative territor...   \n",
       "\n",
       "     latency  input_tokens  output_tokens  \n",
       "0   8.300891          1795            NaN  \n",
       "1  20.916132          1829            NaN  \n",
       "2  20.942740          1714            NaN  \n",
       "3  10.806677          1865            NaN  \n",
       "4  21.112151          1859            NaN  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homeFolder = \"/work/pi_dhruveshpate_umass_edu/project_19/aishwarya/696DS-named-entity-extraction-and-linking-for-KG-construction/code/phi2\"\n",
    "\n",
    "DataPath0 = os.path.join(homeFolder, \"phi2_relations_gt_redocred_dev.csv\")\n",
    "DataPath1 = os.path.join(homeFolder, \"phi2_relations_gt_redocred_dev_v2.csv\")\n",
    "DataPath2 = os.path.join(homeFolder, \"phi2_relations_gt_redocred_dev_v3.csv\")\n",
    "DataPath3 = os.path.join(homeFolder, \"phi2_relations_gt_redocred_dev_v4.csv\")\n",
    "\n",
    "Data0 = pd.read_csv(DataPath0, header=None)\n",
    "Data1 = pd.read_csv(DataPath1, header=None)\n",
    "Data2 = pd.read_csv(DataPath2, header=None)\n",
    "Data3 = pd.read_csv(DataPath3, header=None)\n",
    "Data = pd.concat([Data0, Data1, Data2, Data3], axis=0)\n",
    "\n",
    "Data = Data.rename(columns={0: 'original_index', 1: 'entities', 2: 'latency',\n",
    "                            3: 'input_tokens',  4: 'output_tokens'})\n",
    "\n",
    "Data.to_csv('phi2_relations_gt_redocred_dev_combined.csv', index=False)\n",
    "\n",
    "print(Data.shape, len(list(set(list(Data[\"original_index\"])))))\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24afc088-bb3a-47e0-a8fc-4cb4fbdc2780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b2fa43-de6d-44e9-9154-9c4b9119f201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-hw1]",
   "language": "python",
   "name": "conda-env-.conda-hw1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
