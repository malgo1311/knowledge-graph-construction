{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04afdb07-3e5d-460f-968f-0be2e71fab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ed86e9-5da5-43aa-b0dd-65426d877cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_DATASETS_CACHE'] = '/work/pi_dhruveshpate_umass_edu/amalgonde_umass_edu'\n",
    "os.environ['CONDA_PKGS_DIRS'] = '/work/pi_dhruveshpate_umass_edu/amalgonde_umass_edu'\n",
    "os.environ['CONDA_ENVS_PATH'] = '/work/pi_dhruveshpate_umass_edu/amalgonde_umass_edu'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/work/pi_dhruveshpate_umass_edu/amalgonde_umass_edu'\n",
    "os.environ['HF_HOME'] = '/work/pi_dhruveshpate_umass_edu/amalgonde_umass_edu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f89b9fb-af30-4492-9a0e-91a6d4fe0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f6386-fc84-4c00-b424-e05b67834e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df1e92a-df79-45c2-9a32-c895e232346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/phi-2\"\n",
    "# model_name = \"google/gemma-2b\"\n",
    "\n",
    "# config = AutoConfig.from_pretrained(model_name)\n",
    "# config.max_seq_len = 4096\n",
    "# config.max_answer_len= 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"Model max length - {tokenizer.model_max_length}\")\n",
    "# tokenizer.model_max_length = 4096\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             # max_length=4096,\n",
    "                                             trust_remote_code=True,\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\",\n",
    "                                             # load_in_8bit=True\n",
    "                                            )\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef0502-32b4-454b-83c4-c426c60d2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST\n",
    "\n",
    "# input_text = \"Write me a poem about Machine Learning.\"\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "# # input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, padding='max_length').to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(**input_ids, max_new_tokens=1024)\n",
    "# print(type(tokenizer.decode(outputs[0])), tokenizer.decode(outputs[0])) #.replace('<pad>', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623e9604-1070-4e0a-a116-6ce0196fdf3e",
   "metadata": {},
   "source": [
    "## Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a1f4dab-2ed9-4dec-b0a8-0c5a4d66c8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Triplets</th>\n",
       "      <th>Entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Willi Schneider (skeleton racer)</td>\n",
       "      <td>Wilfried \" Willi \" Schneider (born 13 March 19...</td>\n",
       "      <td>2002 Winter Olympics | start time | 2002\\n2002...</td>\n",
       "      <td>German\\nJeff Pain\\nFIBT World Championships\\n2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ross Alger</td>\n",
       "      <td>Ross Patterson Alger (August 20, 1920  January...</td>\n",
       "      <td>Ross Patterson Alger | place of birth | Prelat...</td>\n",
       "      <td>Rod Sykes\\nOlympic\\nRoss Patterson Alger\\nRoya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Mess of Blues (Jeff Healey album)</td>\n",
       "      <td>Mess of Blues is an album by Jeff Healey. It w...</td>\n",
       "      <td>Mess of Blues | publication date | 2008\\nMess ...</td>\n",
       "      <td>Toronto\\nDoc Pomus\\nCanada\\nStudio 92\\nIslingt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Ramey Idriss</td>\n",
       "      <td>Ramey Idriss (11 September 1911  5 February 19...</td>\n",
       "      <td>Wet Blanket Policy | publication date | 1948\\n...</td>\n",
       "      <td>The Old Chaperone\\nI 'll Wait\\n11 September 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>ELAM (Latin American School of Medicine) Cuba</td>\n",
       "      <td>Escuela Latinoamericana de Medicina (ELAM), fo...</td>\n",
       "      <td>Latin American School of Medicine | country | ...</td>\n",
       "      <td>Guri\\nCuba\\nLatin America\\nEscuela Latinoameri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index                                          Title  \\\n",
       "0           0      0               Willi Schneider (skeleton racer)   \n",
       "1           1      1                                     Ross Alger   \n",
       "2           2      2              Mess of Blues (Jeff Healey album)   \n",
       "3           3      3                                   Ramey Idriss   \n",
       "4           4      4  ELAM (Latin American School of Medicine) Cuba   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Wilfried \" Willi \" Schneider (born 13 March 19...   \n",
       "1  Ross Patterson Alger (August 20, 1920  January...   \n",
       "2  Mess of Blues is an album by Jeff Healey. It w...   \n",
       "3  Ramey Idriss (11 September 1911  5 February 19...   \n",
       "4  Escuela Latinoamericana de Medicina (ELAM), fo...   \n",
       "\n",
       "                                            Triplets  \\\n",
       "0  2002 Winter Olympics | start time | 2002\\n2002...   \n",
       "1  Ross Patterson Alger | place of birth | Prelat...   \n",
       "2  Mess of Blues | publication date | 2008\\nMess ...   \n",
       "3  Wet Blanket Policy | publication date | 1948\\n...   \n",
       "4  Latin American School of Medicine | country | ...   \n",
       "\n",
       "                                            Entities  \n",
       "0  German\\nJeff Pain\\nFIBT World Championships\\n2...  \n",
       "1  Rod Sykes\\nOlympic\\nRoss Patterson Alger\\nRoya...  \n",
       "2  Toronto\\nDoc Pomus\\nCanada\\nStudio 92\\nIslingt...  \n",
       "3  The Old Chaperone\\nI 'll Wait\\n11 September 19...  \n",
       "4  Guri\\nCuba\\nLatin America\\nEscuela Latinoameri...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/work/pi_dhruveshpate_umass_edu/project_19/ReDocREDPreprocessing/Re-DocRED/processed/\"\n",
    "file = \"Re-DocRED_Processed_Dev_EntitiesIncluded.csv\"\n",
    "\n",
    "data = pd.read_csv(os.path.join(data_path, file))\n",
    "# data = data.rename(columns={'Unnamed: 0': 'original_index', 'index': 'original_index'})\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd44a8e8-c119-4d7e-8d8f-e7c387e9a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "entityPredictions = \"/work/pi_dhruveshpate_umass_edu/project_19/aishwarya/696DS-named-entity-extraction-and-linking-for-KG-construction/code/llama2/lamma2_entity_redocred_dev.csv\"\n",
    "\n",
    "dataEntity = pd.read_csv(entityPredictions, header=None)\n",
    "dataEntity = dataEntity.rename(columns={0: 'original_index', 1: \"Entities\", 2: \"InferenceTime\"})\n",
    "print(dataEntity.shape)\n",
    "dataEntity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f9ec3-c0fa-45c8-a62e-12f587084118",
   "metadata": {},
   "outputs": [],
   "source": [
    "completeData = pd.merge(data, dataEntity, on='original_index')\n",
    "print(completeData.shape)\n",
    "completeData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c09b28e-f48a-4980-9ce8-689ef36adf55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2af1a420-7282-4015-a93c-5366bcc0bea3",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa32e25-941b-46e7-b057-bc87d8c90b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/work/pi_dhruveshpate_umass_edu/project_19/ReDocREDPreprocessing/Re-DocRED/processed/\"\n",
    "file_v2 = \"Re-DocRED_Processed_Train.csv\"\n",
    "    \n",
    "def get_example(idx, datatmp):\n",
    "    allEntities = set()\n",
    "    for item in datatmp[\"Triplets\"][idx].split(\"\\n\"):\n",
    "        item = item.split(\" | \")\n",
    "        # print(item, len(item))\n",
    "        if len(item) == 3:\n",
    "            allEntities.add(item[0])\n",
    "            allEntities.add(item[2])\n",
    "\n",
    "    allEntities = list(allEntities)\n",
    "    # print(allEntities)\n",
    "    return datatmp[\"Text\"][idx], allEntities, datatmp[\"Triplets\"][idx]\n",
    "    # return datatmp[\"Text\"][idx], \"; \".join(allEntities), datatmp[\"Triplets\"][idx]\n",
    "    \n",
    "def get_prompt(text, entities):\n",
    "    \n",
    "    data_v2 = pd.read_csv(os.path.join(data_path, file_v2), skiprows = range(1, 2000), nrows = 500)\n",
    "    \n",
    "    ex1, exent1, exout1 = get_example(100, data_v2)\n",
    "    ex2, exent2, exout2 = get_example(200, data_v2)\n",
    "    # ex3, exent3, exout3 = get_example(300, data_v2)\n",
    "    # ex4, exent4, exout4 = get_example(400, data_v2)\n",
    "\n",
    "    prompt=f'''Task Description:\n",
    "The task is to extract Relations between the Entity List for given text, in the form of triplets. \\\n",
    "Extract triplets from the given Text based solely on the relationships present in the text. \\\n",
    "Ensure that entities are chosen directly from the provided Entity List to maintain accuracy. \\\n",
    "Avoid duplicating triplets in the output. Use the provided Example Text and Relations Output as references \\\n",
    "to understand how to identify meaningful relationships between entities from Entity List. \\\n",
    "Pay attention to all potential relations between all the entities and include them in the output.\n",
    "\n",
    "Example Text 1: {ex1}\n",
    "Entity List of Text 1: {exent1}\n",
    "Relations Output of Text 1: {exout1}\n",
    "\n",
    "Example Text 2: {ex2}\n",
    "Entity List of Text 2: {exent2}\n",
    "Relations Output of Text 2: {exout2}\n",
    "\n",
    "Text: {text}\n",
    "Entity List: {entities}\n",
    "Relations Output:'''\n",
    "    \n",
    "#     Example Text 3: {ex3}\n",
    "#     Entity List of Text 3: {exent3}\n",
    "#     Relations Output of Text 3: {exout3}\n",
    "\n",
    "#     Example Text 4: {ex4}\n",
    "#     Entity List of Text 4: {exent4}\n",
    "#     Relations Output of Text 4: {exout4}\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2e4e4-fbba-41c3-aa83-9dc8a219db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = get_prompt(\"text\", \"abd; abc\")\n",
    "len(sample_prompt.split(\" \")) #, sample_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8bec20-c66a-4656-95c5-7a9835b53ce4",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6e2f01-091f-4264-9d69-32f7812fdff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 512\n",
    "\n",
    "# lower the value, deterministic result\n",
    "temperature = 0.1\n",
    "\n",
    "# a higher value increases the chance of finding a better output\n",
    "top_p = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d6b443-d494-405d-87aa-60ee16b3e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class KeywordsStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, keywords_ids:list):\n",
    "        self.keywords = keywords_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        if input_ids[0][-1] in self.keywords:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "# stop_words = ['Test', 'Test Sentence', 'Test Output']\n",
    "stop_words = [\"Example\", \"Entity\", \"Relations\"]\n",
    "stop_ids = [tokenizer.encode(w)[0] for w in stop_words]\n",
    "stop_criteria = KeywordsStoppingCriteria(stop_ids)\n",
    "stop_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2faf8-5c56-427b-ba3e-28b60df148a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_entities_v1(ent):\n",
    "    ent = ent.strip()\n",
    "    ent = ent.split(\"; \")\n",
    "    ent = list(set(ent))\n",
    "    ent = \"; \".join(ent)\n",
    "    return ent\n",
    "\n",
    "def process_gtEntities(triplets):\n",
    "    \n",
    "    if not isinstance(triplets,str):\n",
    "        print(f\"Not string - {triplets}\")\n",
    "            \n",
    "    triplets = triplets.strip()\n",
    "    triplets = triplets.split(\"\\n\")\n",
    "    \n",
    "    output = set()\n",
    "    for t in triplets:\n",
    "        if not isinstance(t,str):\n",
    "            print(f\"Not string - {t}\")\n",
    "            continue\n",
    "            \n",
    "        t = t.split(\" | \")\n",
    "        if len(t) != 3:\n",
    "            continue\n",
    "            \n",
    "        if len(t) != 3:\n",
    "            print(f\"Not len 3 - {t}\")\n",
    "            continue\n",
    "            \n",
    "        output.add(t[0])\n",
    "        output.add(t[2])\n",
    "        \n",
    "    output = list(output)\n",
    "    return output\n",
    "\n",
    "# enty = completeData[\"Entities\"][2]\n",
    "# enty, process_entities(enty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8ca26c-66bd-47eb-9747-c417707f5c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name = \"phi2_relations_gt_redocred_dev_v4.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653001b2-4863-4376-bc02-17cedb767aad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_data = []\n",
    "debug = False\n",
    "\n",
    "for i in tqdm(range(data.shape[0])):\n",
    "    \n",
    "    if i <= 40 and debug:\n",
    "        continue\n",
    "        \n",
    "    if i < 491 or i == 492:\n",
    "        continue\n",
    "        \n",
    "    gt = data['Triplets'][i]\n",
    "    ent = process_gtEntities(data[\"Triplets\"][i])\n",
    "    \n",
    "    # Two step prediction\n",
    "    # prompt = get_prompt(completeData[\"Text\"][i], process_entities_v1(completeData[\"Entities\"][i]))\n",
    "    # gt = completeData['Triplets'][i]\n",
    "    # ent = completeData['Entities'][i]\n",
    "    \n",
    "    # Relation prediction from GT entities\n",
    "    prompt = get_prompt(data[\"Text\"][i], ent)\n",
    "    \n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\") #,  max_length=4096, truncation=True\n",
    "    \n",
    "    start = time.time()\n",
    "    outputs = model.generate(**input_ids,\n",
    "                             max_new_tokens=max_tokens,\n",
    "                             top_p=top_p,\n",
    "                             do_sample=True,\n",
    "                             temperature=temperature,\n",
    "                             pad_token_id=tokenizer.eos_token_id,\n",
    "                             stopping_criteria=StoppingCriteriaList([stop_criteria]),\n",
    "                             )\n",
    "    time_diff = time.time() - start\n",
    "    \n",
    "    # output2 = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    output2 = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "    \n",
    "    if debug:\n",
    "        print(output2)\n",
    "        print(f\"\\nGT : {gt}\")\n",
    "        print(f\"\\nEntities : {ent}\")\n",
    "        \n",
    "    output_data.append([data[\"original_index\"][i], output2, time_diff,\n",
    "                       len(input_ids[0]), len(outputs[0][len(input_ids[0]):])])\n",
    "    \n",
    "    if i % 10 == 0 and i > 1:\n",
    "        with open(output_file_name, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(output_data)\n",
    "            \n",
    "    if debug: break\n",
    "    \n",
    "with open(output_file_name, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849734c2-235c-4f2c-bab0-3f228c7066f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1f87a24-85f7-4170-bbaf-3a99cdb89712",
   "metadata": {},
   "source": [
    "## Combine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c61a996-923c-40ae-aa18-927ecdc66978",
   "metadata": {},
   "outputs": [],
   "source": [
    "homeFolder = \"/work/pi_dhruveshpate_umass_edu/project_19/aishwarya/696DS-named-entity-extraction-and-linking-for-KG-construction/code/phi2\"\n",
    "\n",
    "DataPath0 = os.path.join(homeFolder, \"phi2_relations_gt_redocred_dev.csv\")\n",
    "DataPath1 = os.path.join(homeFolder, \"phi2_relations_gt_redocred_dev_v2.csv\")\n",
    "DataPath2 = os.path.join(homeFolder, \"phi2_relations_gt_redocred_dev_v3.csv\")\n",
    "DataPath3 = os.path.join(homeFolder, \"phi2_relations_gt_redocred_dev_v4.csv\")\n",
    "\n",
    "Data0 = pd.read_csv(DataPath0, header=None)\n",
    "Data1 = pd.read_csv(DataPath1, header=None)\n",
    "Data2 = pd.read_csv(DataPath2, header=None)\n",
    "Data3 = pd.read_csv(DataPath3, header=None)\n",
    "Data = pd.concat([Data0, Data1, Data2, Data3], axis=0)\n",
    "\n",
    "Data = Data.rename(columns={0: 'original_index', 1: 'entities', 2: 'latency',\n",
    "                            3: 'input_tokens',  4: 'output_tokens'})\n",
    "\n",
    "Data.to_csv('phi2_relations_gt_redocred_dev_combined.csv', index=False)\n",
    "\n",
    "print(Data.shape, len(list(set(list(Data[\"original_index\"])))))\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24afc088-bb3a-47e0-a8fc-4cb4fbdc2780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b2fa43-de6d-44e9-9154-9c4b9119f201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-hw1]",
   "language": "python",
   "name": "conda-env-.conda-hw1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
