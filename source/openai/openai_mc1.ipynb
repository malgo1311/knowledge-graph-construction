{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36aef22-9d7d-44e9-9d19-1c44c6bec873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !rm /work/pi_dhruveshpate_umass_edu/project_19/aishwarya/696DS-named-entity-extraction-and-linking-for-KG-construction/code/openai/credentials.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d5e11d-05fb-4046-b761-93ede66a80ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aa073b8-c238-4100-b18d-150340047184",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"credentials.env\")\n",
    "api_key = os.getenv(\"api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "238bee08-9723-4dd1-b153-f97991355afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=api_key)\n",
    "model = \"gpt-4\" # gpt-3.5-turbo-instruct gpt-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db7b22-bb88-4736-b6dc-3115c3f3d070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b6fc15a-631c-4016-9c33-870b28d0cda8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0227a118-e458-4721-88a4-3401eedcebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/work/pi_dhruveshpate_umass_edu/project_19/astha/696DS-named-entity-extraction-and-linking-for-KG-construction/code/mc1/mc1_preprocess/\"\n",
    "file = \"mc1_chunked_data.json\"\n",
    "\n",
    "with open(os.path.join(data_path, file), 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28b8a6fe-08eb-4f87-82dd-dda2d615f4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, dict_keys(['Documents']))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data), data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60325d3e-6e33-4ca8-9aeb-49b4e2f63dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chunk_index': 0,\n",
       "  'chunk_text': 'SOURCE: All News Today\\n\\nTITLE: RALLY SCHEDULED IN SUPPORT OF INCREASED GOVERNMENT ACCOUNTABILITY \\n\\n \\n\\nPUBLISHED: 2012/04/09\\n \\n\\nLOCATION: ABILA, Kronos \\n\\nSilvia Marek, leader of the Protectors of Kronos social action organization is appealing to the citizens of Kronos to join her for a day of peaceful protest tomorrow.\\n\\nMarek asks all concerned citizens to join her to bring awareness to the problem of political corruption and favoritism pervasive in the offices of the government of Kronos.\\n\\n\"I\\'m calling for an end to the heinous corruption, the nepotism and the favoritism displayed by our elected officials\"',\n",
       "  'chunk_tokens': 105}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Documents']['3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ab9ae-6eef-40b8-915a-5d006422a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['historical_document']['reports'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540bd782-d611-4367-9d86-7311caefa218",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### RElations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b13160-cc59-4085-b191-8a6612e7d537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1339, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_key</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>entities</th>\n",
       "      <th>latency</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "      <th>processed_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Kyrla Halford; University of Abila; Kronos; Oc...</td>\n",
       "      <td>4.019342</td>\n",
       "      <td>713</td>\n",
       "      <td>103</td>\n",
       "      <td>Kyrla Halford; University of Abila; Kronos; Oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Master’s Thesis; Paper SW1138; Esme Nesmith; E...</td>\n",
       "      <td>1.967966</td>\n",
       "      <td>705</td>\n",
       "      <td>38</td>\n",
       "      <td>Master’s Thesis; Paper SW1138; Esme Nesmith; E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1997; Elodis; Kronos; Tiskele River; Protector...</td>\n",
       "      <td>1.920458</td>\n",
       "      <td>690</td>\n",
       "      <td>31</td>\n",
       "      <td>1997; Elodis; Kronos; Tiskele River; Protector...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1997; Elodis; Elodis Community Health Center; ...</td>\n",
       "      <td>3.157337</td>\n",
       "      <td>690</td>\n",
       "      <td>19</td>\n",
       "      <td>1997; Elodis; Elodis Community Health Center; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Initial Stage; Grassroots Effort; Council; Elo...</td>\n",
       "      <td>2.945101</td>\n",
       "      <td>733</td>\n",
       "      <td>70</td>\n",
       "      <td>Initial Stage; Grassroots Effort; Council; Elo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_key  chunk_index                                           entities  \\\n",
       "0        0            0  Kyrla Halford; University of Abila; Kronos; Oc...   \n",
       "1        0            1  Master’s Thesis; Paper SW1138; Esme Nesmith; E...   \n",
       "2        0            2  1997; Elodis; Kronos; Tiskele River; Protector...   \n",
       "3        0            3  1997; Elodis; Elodis Community Health Center; ...   \n",
       "4        0            4  Initial Stage; Grassroots Effort; Council; Elo...   \n",
       "\n",
       "    latency  input_tokens  output_tokens  \\\n",
       "0  4.019342           713            103   \n",
       "1  1.967966           705             38   \n",
       "2  1.920458           690             31   \n",
       "3  3.157337           690             19   \n",
       "4  2.945101           733             70   \n",
       "\n",
       "                                  processed_entities  \n",
       "0  Kyrla Halford; University of Abila; Kronos; Oc...  \n",
       "1  Master’s Thesis; Paper SW1138; Esme Nesmith; E...  \n",
       "2  1997; Elodis; Kronos; Tiskele River; Protector...  \n",
       "3  1997; Elodis; Elodis Community Health Center; ...  \n",
       "4  Initial Stage; Grassroots Effort; Council; Elo...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_data = pd.read_csv(\"openai_mc1_entities_Apr13_processed.csv\")\n",
    "# Data0 = Data0.rename(columns={0: 'doc_key', 1: 'chunk_index', 2: 'entities', 3: 'latency',\n",
    "#                             4: 'input_tokens',  5: 'output_tokens'})\n",
    "print(entities_data.shape)\n",
    "entities_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2db11365-7e1c-4518-8f22-bd506c781d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_key</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>entities</th>\n",
       "      <th>latency</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "      <th>processed_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>All News Today; 2012/04/09; ABILA; Kronos; Sil...</td>\n",
       "      <td>1.462747</td>\n",
       "      <td>658</td>\n",
       "      <td>28</td>\n",
       "      <td>All News Today; 2012/04/09; ABILA; Kronos; Sil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_key  chunk_index                                           entities  \\\n",
       "32        3            0  All News Today; 2012/04/09; ABILA; Kronos; Sil...   \n",
       "\n",
       "     latency  input_tokens  output_tokens  \\\n",
       "32  1.462747           658             28   \n",
       "\n",
       "                                   processed_entities  \n",
       "32  All News Today; 2012/04/09; ABILA; Kronos; Sil...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_data.loc[lambda df: df.doc_key == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cf34e87-f82e-49ab-b481-ce90d4b26db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json = {}\n",
    "for idx in range(entities_data.shape[0]):\n",
    "    doc_key = entities_data[\"doc_key\"][idx]\n",
    "    chunk_index = entities_data[\"chunk_index\"][idx]\n",
    "    processed_entities = entities_data[\"processed_entities\"][idx]\n",
    "    \n",
    "    if doc_key not in data_json:\n",
    "        data_json[doc_key] = {}\n",
    "        \n",
    "    data_json[doc_key][chunk_index] = processed_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51d97b9e-a1ee-4e68-9adb-b14b1db5e3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'All News Today; 2012/04/09; ABILA; Kronos; Silvia Marek; Protectors of Kronos'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_json[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256edf00-a02c-4e3f-9837-fbd5bcbc88c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43cb8b5-596b-422b-b318-c5c782840e36",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Entities Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21fa8f4-3009-47d2-9d84-abfd8a27241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/work/pi_dhruveshpate_umass_edu/project_19/ReDocREDPreprocessing/Re-DocRED/processed/\"\n",
    "file_v2 = \"Re-DocRED_Processed_Train.csv\"\n",
    "    \n",
    "def get_entity_example(idx, datatmp):\n",
    "    allEntities = set()\n",
    "    for item in datatmp[\"Triplets\"][idx].split(\"\\n\"):\n",
    "        item = item.split(\" | \")\n",
    "        if len(item) == 3:\n",
    "            allEntities.add(item[0])\n",
    "            allEntities.add(item[2])\n",
    "\n",
    "    return datatmp[\"Text\"][idx], '; '.join(list(allEntities))\n",
    "    \n",
    "def get_entities_prompt(text):\n",
    "    \n",
    "    data_v2 = pd.read_csv(os.path.join(data_path, file_v2), skiprows = range(1, 2000), nrows = 500)\n",
    "    \n",
    "    ex1, exout1 = get_entity_example(1, data_v2)\n",
    "    ex2, exout2 = get_entity_example(2, data_v2)\n",
    "\n",
    "    prompt=f'''Task: Please detect all the entities from the given input Text.\n",
    "Entities could be people, organization, places, concepts, dates or any other proper nouns present in the text. \\\n",
    "Use the following examples as reference to understand the task. \\\n",
    "Give the output in the same format as given in the Example Entities Output, i.e., separated by a semicolon, ';'.\n",
    "\n",
    "Example Text 1: {ex1}\n",
    "Example Entities Output 1: {exout1}\n",
    "\n",
    "Example Text 2: {ex2}\n",
    "Example Entities Output 2: {exout2}\n",
    "\n",
    "Text: {text}\n",
    "Entities Output:'''\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def get_system_msg():\n",
    "    \n",
    "    data_v2 = pd.read_csv(os.path.join(data_path, file_v2), skiprows = range(1, 2000), nrows = 500)\n",
    "    \n",
    "    ex1, exout1 = get_entity_example(1, data_v2)\n",
    "    ex2, exout2 = get_entity_example(2, data_v2)\n",
    "\n",
    "    prompt=f'''You are a helpful assistant and an expert in named entity extraction.\n",
    "Task: Please detect all the entities from the given input Text.\n",
    "Entities could be people, organization, places, concepts, dates or any other proper nouns present in the text. \\\n",
    "Use the following examples as reference to understand the task. \\\n",
    "Give the output in the same format as given in the Example Entities Output, i.e., separated by a semicolon, ';'.\n",
    "\n",
    "Example Text 1: {ex1}\n",
    "Example Entities Output 1: {exout1}\n",
    "\n",
    "Example Text 2: {ex2}\n",
    "Example Entities Output 2: {exout2}'''\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def get_content(text):\n",
    "    \n",
    "    prompt=f'''Text: {text}\n",
    "Entities Output:'''\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ee0a58-a03d-45ca-958d-4a614c318d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_system_msg())\n",
    "print(get_content(\"hey\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a13b8-f380-49da-9f9d-a0949eeefd15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Relations Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f545d243-8478-4867-8d0c-0fc48d530cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/work/pi_dhruveshpate_umass_edu/project_19/ReDocREDPreprocessing/Re-DocRED/processed/\"\n",
    "file_v2 = \"Re-DocRED_Processed_Train.csv\"\n",
    "    \n",
    "def get_example(idx, datatmp):\n",
    "    allEntities = set()\n",
    "    for item in datatmp[\"Triplets\"][idx].split(\"\\n\"):\n",
    "        item = item.split(\" | \")\n",
    "        # print(item, len(item))\n",
    "        if len(item) == 3:\n",
    "            allEntities.add(item[0])\n",
    "            allEntities.add(item[2])\n",
    "\n",
    "    allEntities = list(allEntities)\n",
    "    # print(allEntities)\n",
    "    # return datatmp[\"Text\"][idx], allEntities, datatmp[\"Triplets\"][idx]\n",
    "    return datatmp[\"Text\"][idx], \"; \".join(allEntities), datatmp[\"Triplets\"][idx]\n",
    "    \n",
    "def get_relations_prompt(text, entities):\n",
    "    \n",
    "    data_v2 = pd.read_csv(os.path.join(data_path, file_v2), skiprows = range(1, 2000), nrows = 500)\n",
    "    \n",
    "    ex1, exent1, exout1 = get_example(1, data_v2)\n",
    "    ex2, exent2, exout2 = get_example(2, data_v2)\n",
    "\n",
    "    prompt=f'''Task Description:\n",
    "The task is to extract Relations between the Entity List for given text, in the form of triplets. \\\n",
    "Extract triplets from the given Text based solely on the relationships present in the text. \\\n",
    "Ensure that entities are chosen directly from the provided Entity List to maintain accuracy. \\\n",
    "Avoid duplicating triplets in the output. Use the provided Example Text and Relations Output as references \\\n",
    "to understand how to identify meaningful relationships between entities from Entity List. \\\n",
    "Pay attention to all potential relations between all the entities and include them in the output.\n",
    "\n",
    "Example Text 1: {ex1}\n",
    "Entity List of Text 1: {exent1}\n",
    "Relations Output of Text 1: {exout1}\n",
    "\n",
    "Example Text 2: {ex2}\n",
    "Entity List of Text 2: {exent2}\n",
    "Relations Output of Text 2: {exout2}\n",
    "\n",
    "Text: {text}\n",
    "Entity List: {entities}\n",
    "Relations Output:'''\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e16651c-52a6-468d-b318-0391b6548ea0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Description:\n",
      "The task is to extract Relations between the Entity List for given text, in the form of triplets. Extract triplets from the given Text based solely on the relationships present in the text. Ensure that entities are chosen directly from the provided Entity List to maintain accuracy. Avoid duplicating triplets in the output. Use the provided Example Text and Relations Output as references to understand how to identify meaningful relationships between entities from Entity List. Pay attention to all potential relations between all the entities and include them in the output.\n",
      "\n",
      "Example Text 1: Udawalawe National Park lies on the boundary of Sabaragamuwa and Uva Provinces, in Sri Lanka. The national park was created to provide a sanctuary for wild animals displaced by the construction of the Udawalawe Reservoir on the Walawe River, as well as to protect the catchment of the reservoir. The reserve covers of land area and was established on 30 June 1972. Before the designation of the national park, the area was used for shifting cultivation (chena farming). The farmers were gradually removed once the national park was declared. The park is from Colombo. Udawalawe is an important habitat for water birds and Sri Lankan elephants. It is a popular tourist destination and the third most visited park in the country. \n",
      "Entity List of Text 1: Colombo; Sabaragamuwa; Udawalawe National Park; 30 June 1972; Udawalawe Reservoir; Sri Lanka; Uva Provinces; Walawe River\n",
      "Relations Output of Text 1: Sabaragamuwa | located in the administrative territorial entity | Sri Lanka\n",
      "Sabaragamuwa | country | Sri Lanka\n",
      "Sri Lanka | contains administrative territorial entity | Sabaragamuwa\n",
      "Sri Lanka | contains administrative territorial entity | Uva Provinces\n",
      "Walawe River | country | Sri Lanka\n",
      "Udawalawe National Park | located in the administrative territorial entity | Sabaragamuwa\n",
      "Udawalawe National Park | country | Sri Lanka\n",
      "Udawalawe National Park | inception | 30 June 1972\n",
      "Colombo | country | Sri Lanka\n",
      "Udawalawe Reservoir | country | Sri Lanka\n",
      "Uva Provinces | country | Sri Lanka\n",
      "Uva Provinces | located in the administrative territorial entity | Sri Lanka\n",
      "Walawe River | located in the administrative territorial entity | Sri Lanka\n",
      "Udawalawe National Park | located in the administrative territorial entity | Sri Lanka\n",
      "Colombo | located in the administrative territorial entity | Sri Lanka\n",
      "Udawalawe Reservoir | located in the administrative territorial entity | Sri Lanka\n",
      "\n",
      "\n",
      "Example Text 2: The Road to Ruin is a 1970 album released by husband and wife John and Beverley Martyn. It was the second (and last) album released as a duo. Island Records persuaded John Martyn to resume his solo career as they believed that the public was more interested in John as a solo artist rather than as part of a duo. The album marked the first collaboration on record between John and bassist Danny Thompson, who featured on many of Martyn's subsequent recordings. The album's first track \" Primrose Hill \" written and sung by Beverley Martyn, and featuring Ray Warleigh on saxophone, about the simple joys of domesticity, was extensively sampled by Fatboy Slim for the track \" North West Three \" from his 2004 album Palookaville. \n",
      "Entity List of Text 2: 2004; Palookaville; 1970; Danny Thompson; Island Records; Beverley Martyn; The Road to Ruin; Fatboy Slim; Martyn; John; Primrose Hill; Road to Ruin\n",
      "Relations Output of Text 2: John | record label | Island Records\n",
      "John | spouse | Beverley Martyn\n",
      "Beverley Martyn | spouse | John\n",
      "Beverley Martyn | record label | Island Records\n",
      "Beverley Martyn | spouse | Martyn\n",
      "Road to Ruin | publication date | 1970\n",
      "Road to Ruin | record label | Island Records\n",
      "Martyn | record label | Island Records\n",
      "Palookaville | publication date | 2004\n",
      "Palookaville | performer | Fatboy Slim\n",
      "Primrose Hill | performer | Beverley Martyn\n",
      "Primrose Hill | part of | The Road to Ruin\n",
      "The Road to Ruin | publication date | 1970\n",
      "Primrose Hill | publication date | 1970\n",
      "The Road to Ruin | performer | Martyn\n",
      "Danny Thompson | record label | Island Records\n",
      "Primrose Hill | record label | Island Records\n",
      "Road to Ruin | performer | John\n",
      "The Road to Ruin | performer | John\n",
      "The Road to Ruin | record label | Island Records\n",
      "Martyn | spouse | Beverley Martyn\n",
      "Fatboy Slim | notable work | Palookaville\n",
      "Beverley Martyn | notable work | Primrose Hill\n",
      "The Road to Ruin | has part | Primrose Hill\n",
      "Martyn | notable work | The Road to Ruin\n",
      "John | notable work | Road to Ruin\n",
      "John | notable work | The Road to Ruin\n",
      "\n",
      "\n",
      "Text: text\n",
      "Entity List: entities\n",
      "Relations Output:\n"
     ]
    }
   ],
   "source": [
    "print(get_relations_prompt(\"text\", \"entities\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9641eefd-8124-4300-b6ac-54155a5196cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad580a-807f-4ec1-8622-4997755e9d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_entities_prompt(\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395e85e4-52d8-419a-8f73-f6aa27572d6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae783b55-07a1-4d5f-bc02-9057939342d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(text):\n",
    "    \n",
    "    prompt = get_entities_prompt(text)\n",
    "    \n",
    "    start = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature = 0,\n",
    "        max_tokens = 256,\n",
    "    )\n",
    "    time_diff = time.time() - start\n",
    "    \n",
    "    return response, time_diff\n",
    "\n",
    "def get_chat_prediction(messages):\n",
    "    \n",
    "    start = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature = 0,\n",
    "        max_tokens = 256,\n",
    "    )\n",
    "    time_diff = time.time() - start\n",
    "    \n",
    "    return response, time_diff\n",
    "\n",
    "\n",
    "def get_relation_prediction(text, entities):\n",
    "    \n",
    "    prompt = get_relations_prompt(text, entities)\n",
    "    \n",
    "    start = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    time_diff = time.time() - start\n",
    "    \n",
    "    return response, time_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bec205d4-b5d4-42c2-ad7c-91afe5ca81bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = []\n",
    "output_filename = \"openai_mc1_triplets_Apr13.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f41e37d-c082-473e-a297-2abb86400741",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679fb154-3ab1-4806-870b-de37e6e11ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "\n",
    "all_keys = list(data['Documents'].keys())\n",
    "\n",
    "for i in tqdm(range(len(all_keys))):\n",
    "    \n",
    "    doc_key = all_keys[i]\n",
    "    \n",
    "    if doc_key != '5' and debug:\n",
    "        continue\n",
    "    \n",
    "    # messages = [{\"role\": \"system\", \"content\": get_system_msg()}]\n",
    "    \n",
    "    for item in data['Documents'][i]:\n",
    "        \n",
    "        # print(messages)\n",
    "    \n",
    "        #\n",
    "        text = item['chunk_text']\n",
    "        chunk_idx = item['chunk_index']\n",
    "        \n",
    "        # messages.append({\"role\": \"user\", \"content\": get_content(text)})\n",
    "        \n",
    "        output, time_diff = get_prediction(text)\n",
    "        # output, time_diff = get_chat_prediction(messages)\n",
    "        # output, time_diff = get_relation_prediction(i)\n",
    "\n",
    "        outputText = output.choices[0].message.content\n",
    "        input_tokens = output.usage.prompt_tokens\n",
    "        output_tokens = output.usage.completion_tokens\n",
    "        \n",
    "        # messages.append({\"role\": \"assistant\", \"content\": outputText})\n",
    "\n",
    "        if debug:\n",
    "            print(f\"\\nInput - {text}\")\n",
    "            # print(f\"\\nGT - {data['Triplets'][i]}\")\n",
    "            print(f\"\\noutput - {outputText}\")\n",
    "\n",
    "        output_data.append([doc_key, chunk_idx, outputText, time_diff,\n",
    "                           input_tokens, output_tokens])\n",
    "\n",
    "        if i % 10 == 0 and i > 1:\n",
    "\n",
    "            with open(output_filename, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerows(output_data)\n",
    "                \n",
    "        # if debug: break\n",
    "        \n",
    "    if debug: break\n",
    "    \n",
    "    with open(output_filename, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21755fa2-fff9-4927-8fa1-2ff95441c619",
   "metadata": {},
   "source": [
    "### Relation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5356e2cb-78bc-4862-ac6e-1b117fca526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "292579ae-7486-403a-933a-184cc147fda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('0', 0), ('0', 1), ('0', 2), ('0', 3), ('0', 4), ('0', 5)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bbe4f2f-a3ef-4ac4-83b6-853b71441b0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 847/847 [00:06<00:00, 128.34it/s]\n"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "\n",
    "all_keys = list(data['Documents'].keys())\n",
    "\n",
    "for i in tqdm(range(len(all_keys))):\n",
    "    \n",
    "    doc_key = all_keys[i]\n",
    "    \n",
    "    if doc_key != '3' and debug:\n",
    "        continue\n",
    "        \n",
    "    if doc_key != '400':\n",
    "        continue\n",
    "    \n",
    "    # messages = [{\"role\": \"system\", \"content\": get_system_msg()}]\n",
    "    \n",
    "    for item in data['Documents'][doc_key]:\n",
    "        \n",
    "        # print(doc_key, item)\n",
    "        \n",
    "        # print(messages)\n",
    "    \n",
    "        #\n",
    "        text = item['chunk_text']\n",
    "        chunk_idx = item['chunk_index']\n",
    "        \n",
    "        # print(doc_key, chunk_idx)\n",
    "        \n",
    "        \n",
    "        # if (doc_key, chunk_idx) != ('3',0):\n",
    "        #     continue\n",
    "            \n",
    "        # if (doc_key, chunk_idx) in processed:\n",
    "        #     continue\n",
    "        # else:\n",
    "        #     processed.add((doc_key, chunk_idx))\n",
    "        \n",
    "        # if int(chunk_index) in data_json[int(doc_key)]:\n",
    "        processed_entities = data_json[int(doc_key)][chunk_idx]\n",
    "\n",
    "        # messages.append({\"role\": \"user\", \"content\": get_content(text)})\n",
    "\n",
    "        # output, time_diff = get_prediction(text)\n",
    "        # output, time_diff = get_chat_prediction(messages)\n",
    "        output, time_diff = get_relation_prediction(i, processed_entities)\n",
    "\n",
    "        outputText = output.choices[0].message.content\n",
    "        input_tokens = output.usage.prompt_tokens\n",
    "        output_tokens = output.usage.completion_tokens\n",
    "\n",
    "        # messages.append({\"role\": \"assistant\", \"content\": outputText})\n",
    "\n",
    "        if debug:\n",
    "            print(f\"\\nInput - {text}\")\n",
    "            # print(f\"\\nGT - {data['Triplets'][i]}\")\n",
    "            print(f\"\\noutput - {outputText}\")\n",
    "\n",
    "        output_data.append([doc_key, chunk_idx, outputText, time_diff,\n",
    "                           input_tokens, output_tokens])\n",
    "            \n",
    "        # else:\n",
    "        #     output_data.append([doc_key, chunk_idx, \"No data\", 0,\n",
    "        #                        0, 0])\n",
    "\n",
    "        if i % 10 == 0 and i > 1:\n",
    "\n",
    "            with open(output_filename, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerows(output_data)\n",
    "                \n",
    "        if debug: break\n",
    "        \n",
    "    if debug: break\n",
    "    \n",
    "    with open(output_filename, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5370201-260d-41db-a3cb-f95627353b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['400',\n",
       " 0,\n",
       " 'News Online Today | publication date | 2011/05/15\\nABILA | country | Kronos\\nProtectors of Kronos | country | Kronos\\nDRUG RELATED ARRESTS ON THE RISE | publication date | 2011/05/15\\nDRUG RELATED ARRESTS ON THE RISE | publisher | News Online Today\\nProtectors of Kronos | located in the administrative territorial entity | Kronos\\nABILA | located in the administrative territorial entity | Kronos\\nNews Online Today | located in the administrative territorial entity | ABILA\\nNews Online Today | country | Kronos\\nDRUG RELATED ARRESTS ON THE RISE | located in the administrative territorial entity | ABILA\\nDRUG RELATED ARRESTS ON THE RISE | country | Kronos',\n",
       " 6.3870508670806885,\n",
       " 1125,\n",
       " 161]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c793e651-fb4a-4b0d-a204-900e14736731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chunk_index': 0,\n",
       "  'chunk_text': \"SOURCE: News Online Today\\n\\nTITLE: KRONOS GOVERNMENT PLANS TAX BREAKS TO ENCOURAGE FOREIGN INVESTMENT \\n\\nPUBLISHED: 1992/12/12\\n \\nLOCATION: ABILA, Kronos \\n\\nThe Government of Kronos is currently planning some of the world's most generous tax breaks for gas investments.\\n\\nThe Government of Kronos is pitching the plan as a win-win situation given that the high-income countries will be struggling to discover and exploit ever-dwindling supplies of fossil-fuels and the developing Kronos needs income, infrastructure, and cheap energy.\",\n",
       "  'chunk_tokens': 85}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Documents']['399']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd80df16-ea9d-47b0-b289-17d0da7ea236",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb4ecffe-b07a-49f2-8e94-ff7ae15e8ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.remove(('3',0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e758390-92f1-4fb2-b993-b12c27108a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3',\n",
       " 1,\n",
       " 'All News Today; 2012/04/09; ABILA; Kronos; Silvia Marek; Protectors of Kronos')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_key, chunk_index, data_json[3][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaa2ed3-500f-4d0c-a492-d8f4400863c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Process Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed57ad1-38ae-4884-aee5-17b105ba2dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data0 = pd.read_csv(\"openai_mc1_entities_Apr13.csv\", header=None)\n",
    "Data0 = Data0.rename(columns={0: 'doc_key', 1: 'chunk_index', 2: 'entities', 3: 'latency',\n",
    "                            4: 'input_tokens',  5: 'output_tokens'})\n",
    "Data0[\"processed_entities\"] = \"\"\n",
    "print(Data0.shape)\n",
    "\n",
    "Data0 = Data0.iloc[9:]\n",
    "Data0 = Data0.reset_index(drop=True)\n",
    "print(Data0.shape)\n",
    "\n",
    "Data0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115137d-0b0e-41b0-a757-dff9e44144de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i in range(Data0.shape[0]):\n",
    "    \n",
    "    # if i != 261: # 1, 5, 27\n",
    "    #     continue\n",
    "        \n",
    "    ent = Data0[\"entities\"][i]\n",
    "    ent2 = ent.split(\"; \")\n",
    "    \n",
    "    ent3 = []\n",
    "    for item in ent2:\n",
    "        item = item.strip()\n",
    "        if item and item not in ent3:\n",
    "            ent3.append(item)\n",
    "    ent3 = \"; \".join(ent3)\n",
    "    \n",
    "#     if ent != ent3:\n",
    "\n",
    "#         print(f\"\\n{i}\")\n",
    "#         print(ent)\n",
    "#         print(f\"\\n{ent3}\")\n",
    "#         print(\"*\"*20)\n",
    "    \n",
    "    Data0[\"processed_entities\"][i] = ent3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142611e3-573d-4f54-b96f-e812210f843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data0.head()\n",
    "Data0.to_csv('openai_mc1_entities_Apr13_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe4085-690f-4499-9cf1-1d85594f30f5",
   "metadata": {},
   "source": [
    "## Combine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a77a17-7293-429f-bb63-f737f43f09c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_key</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>triplets</th>\n",
       "      <th>latency</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Kyrla Halford | education | University of Abil...</td>\n",
       "      <td>18.778480</td>\n",
       "      <td>1194</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Esme Nesmith | author | Master’s Thesis\\nElian...</td>\n",
       "      <td>4.921607</td>\n",
       "      <td>1129</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Protectors of Kronos | publication date | 1997...</td>\n",
       "      <td>8.510802</td>\n",
       "      <td>1123</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Elodis Community Health Center | inception | 1...</td>\n",
       "      <td>5.819815</td>\n",
       "      <td>1111</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Initial Stage | participant | Grassroots Effor...</td>\n",
       "      <td>11.607945</td>\n",
       "      <td>1161</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_key  chunk_index                                           triplets  \\\n",
       "0        0            0  Kyrla Halford | education | University of Abil...   \n",
       "1        0            1  Esme Nesmith | author | Master’s Thesis\\nElian...   \n",
       "2        0            2  Protectors of Kronos | publication date | 1997...   \n",
       "3        0            3  Elodis Community Health Center | inception | 1...   \n",
       "4        0            4  Initial Stage | participant | Grassroots Effor...   \n",
       "\n",
       "     latency  input_tokens  output_tokens  \n",
       "0  18.778480          1194            498  \n",
       "1   4.921607          1129            128  \n",
       "2   8.510802          1123            214  \n",
       "3   5.819815          1111            145  \n",
       "4  11.607945          1161            313  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homeFolder = \"/work/pi_dhruveshpate_umass_edu/project_19/aishwarya/696DS-named-entity-extraction-and-linking-for-KG-construction/code/openai\"\n",
    "\n",
    "main_output = 'openai_mc1_triplets_combined.csv'\n",
    "\n",
    "DataPath0 = os.path.join(homeFolder, \"openai_mc1_triplets_Apr13.csv\")\n",
    "DataPath1 = os.path.join(homeFolder, \"openai_mc1_triplets_Apr13_400.csv\")\n",
    "# DataPath2 = os.path.join(homeFolder, \"openai_relations_gt_redocred_dev_v5.csv\")\n",
    "# DataPath3 = os.path.join(homeFolder, \"openai_relations_gt_redocred_dev_v6.csv\")\n",
    "\n",
    "Data0 = pd.read_csv(DataPath0, header=None)\n",
    "Data1 = pd.read_csv(DataPath1, header=None)\n",
    "# Data2 = pd.read_csv(DataPath2, header=None)\n",
    "# Data3 = pd.read_csv(DataPath3, header=None)\n",
    "Data = pd.concat([Data0, Data1], axis=0)\n",
    "\n",
    "Data = Data.rename(columns={0: 'doc_key', 1: 'chunk_index', 2: 'triplets', 3: 'latency',\n",
    "                           4: 'input_tokens',  5: 'output_tokens', 6: 'article_name'})\n",
    "\n",
    "Data.to_csv(main_output, index=False)\n",
    "\n",
    "# print(Data.shape, len(list(set(list(Data[\"original_index\"])))))\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1c8c41-e7b8-4702-87e6-f5cda163563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(list(Data[\"original_index\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1500986-eaa1-4e8a-a59d-fef1f4645cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_id = list(set(list(Data[\"original_index\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d86be8-27b1-4b34-9185-57828a9535d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "109ec93b-d046-4e77-8023-dfc05bb32907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87480f58-c59a-414a-a179-3203d459c1c1",
   "metadata": {},
   "source": [
    "## Analyse output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a9e12-ea69-463c-b6e4-56ede1b11b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "homeFolder = \"/work/pi_dhruveshpate_umass_edu/project_19/aishwarya/696DS-named-entity-extraction-and-linking-for-KG-construction/code/openai\"\n",
    "\n",
    "DataPath0 = 'openai_relations_gt_redocred_dev_all.csv'\n",
    "\n",
    "Data0 = pd.read_csv(DataPath0)\n",
    "\n",
    "Data0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2bbe7-a45e-45bd-a936-c66507ef629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data0['entities'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace7ad0e-06ee-4dfa-92d6-a84899930a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-hw1]",
   "language": "python",
   "name": "conda-env-.conda-hw1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
